{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import LlamaCpp\n",
    "from langchain import PromptTemplate, LLMChain\n",
    "from langchain.callbacks.manager import CallbackManager\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Callbacks support token-wise streaming\n",
    "callback_manager = CallbackManager([StreamingStdOutCallbackHandler()])\n",
    "# Verbose is required to pass to the callback manager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"/Users/q616967/Workspace/llama.cpp/llama-2-13b-chat.ggmlv3.q4_0.bin\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama.cpp: loading model from /Users/q616967/Workspace/llama.cpp/llama-2-13b-chat.ggmlv3.q4_0.bin\n",
      "llama_model_load_internal: format     = ggjt v3 (latest)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 512\n",
      "llama_model_load_internal: n_embd     = 5120\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 40\n",
      "llama_model_load_internal: n_head_kv  = 40\n",
      "llama_model_load_internal: n_layer    = 40\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: n_gqa      = 1\n",
      "llama_model_load_internal: rnorm_eps  = 1.0e-06\n",
      "llama_model_load_internal: n_ff       = 13824\n",
      "llama_model_load_internal: freq_base  = 10000.0\n",
      "llama_model_load_internal: freq_scale = 1\n",
      "llama_model_load_internal: ftype      = 2 (mostly Q4_0)\n",
      "llama_model_load_internal: model size = 13B\n",
      "llama_model_load_internal: ggml ctx size =    0.11 MB\n",
      "llama_model_load_internal: mem required  = 7349.72 MB (+  400.00 MB per state)\n",
      "llama_new_context_with_model: kv self size  =  400.00 MB\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: using MPS\n",
      "ggml_metal_init: loading '/Users/q616967/Workspace/bmw/xai_sofia_casadei_master_thesis/thesis-venv/lib/python3.10/site-packages/llama_cpp/ggml-metal.metal'\n",
      "ggml_metal_init: loaded kernel_add                            0x133a73a40\n",
      "ggml_metal_init: loaded kernel_add_row                        0x133a73ca0\n",
      "ggml_metal_init: loaded kernel_mul                            0x133a73f00\n",
      "ggml_metal_init: loaded kernel_mul_row                        0x133a74160\n",
      "ggml_metal_init: loaded kernel_scale                          0x133a743c0\n",
      "ggml_metal_init: loaded kernel_silu                           0x133a74620\n",
      "ggml_metal_init: loaded kernel_relu                           0x133a74880\n",
      "ggml_metal_init: loaded kernel_gelu                           0x133a74ae0\n",
      "ggml_metal_init: loaded kernel_soft_max                       0x133a74d40\n",
      "ggml_metal_init: loaded kernel_diag_mask_inf                  0x133a74fa0\n",
      "ggml_metal_init: loaded kernel_get_rows_f16                   0x133a75200\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_0                  0x133a75460\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_1                  0x133a756c0\n",
      "ggml_metal_init: loaded kernel_get_rows_q2_K                  0x133a75920\n",
      "ggml_metal_init: loaded kernel_get_rows_q3_K                  0x133a75b80\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_K                  0x133a75de0\n",
      "ggml_metal_init: loaded kernel_get_rows_q5_K                  0x133a76040\n",
      "ggml_metal_init: loaded kernel_get_rows_q6_K                  0x133a762a0\n",
      "ggml_metal_init: loaded kernel_rms_norm                       0x133a76500\n",
      "ggml_metal_init: loaded kernel_norm                           0x133a76760\n",
      "ggml_metal_init: loaded kernel_mul_mat_f16_f32                0x133a769c0\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_0_f32               0x133a76c20\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_1_f32               0x133a76e80\n",
      "ggml_metal_init: loaded kernel_mul_mat_q2_K_f32               0x133a770e0\n",
      "ggml_metal_init: loaded kernel_mul_mat_q3_K_f32               0x133a77340\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_K_f32               0x133a775a0\n",
      "ggml_metal_init: loaded kernel_mul_mat_q5_K_f32               0x133a77800\n",
      "ggml_metal_init: loaded kernel_mul_mat_q6_K_f32               0x133a77a60\n",
      "ggml_metal_init: loaded kernel_rope                           0x133a77cc0\n",
      "ggml_metal_init: loaded kernel_alibi_f32                      0x133a77f20\n",
      "ggml_metal_init: loaded kernel_cpy_f32_f16                    0x133a78180\n",
      "ggml_metal_init: loaded kernel_cpy_f32_f32                    0x133a783e0\n",
      "ggml_metal_init: loaded kernel_cpy_f16_f16                    0x133a78640\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize = 10922.67 MB\n",
      "ggml_metal_init: hasUnifiedMemory             = true\n",
      "ggml_metal_init: maxTransferRate              = built-in GPU\n",
      "llama_new_context_with_model: max tensor size =    87.89 MB\n",
      "ggml_metal_add_buffer: allocated 'data            ' buffer, size =  6984.06 MB, (14738.81 / 10922.67), warning: current allocated size is greater than the recommended max working set size\n",
      "ggml_metal_add_buffer: allocated 'eval            ' buffer, size =    12.00 MB, (14750.81 / 10922.67), warning: current allocated size is greater than the recommended max working set size\n",
      "ggml_metal_add_buffer: allocated 'kv              ' buffer, size =   402.00 MB, (15152.81 / 10922.67), warning: current allocated size is greater than the recommended max working set size\n",
      "ggml_metal_add_buffer: allocated 'scr0            ' buffer, size =   162.00 MB, (15314.81 / 10922.67), warning: current allocated size is greater than the recommended max working set size\n",
      "ggml_metal_add_buffer: allocated 'scr1            ' buffer, size =   192.00 MBAVX = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | VSX = 0 | \n",
      ", (15506.81 / 10922.67), warning: current allocated size is greater than the recommended max working set size\n"
     ]
    }
   ],
   "source": [
    "n_gpu_layers = 1  # Change this value based on your model and your GPU VRAM pool.\n",
    "    # 1 is enough for Metal installation\n",
    "n_batch = 512  # Should be between 1 and n_ctx, consider the amount of VRAM in your GPU.\n",
    "    # defaults to 8\n",
    "\n",
    "# Make sure the model path is correct for your system!\n",
    "llm = LlamaCpp(\n",
    "    model_path=model_path,\n",
    "    n_gpu_layers=n_gpu_layers,\n",
    "    n_batch=n_batch,\n",
    "    n_threads=8,\n",
    "    f16_kv=True,  # MUST set to True, otherwise you will run into problem after a couple of calls\n",
    "    temperature=0.0,\n",
    "    max_tokens=-1,\n",
    "    repeat_penalty=1,\n",
    "    streaming=True,\n",
    "    callback_manager=callback_manager,\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# previous\n",
    "template = \"\"\"Question: {question}\n",
    "\n",
    "Answer: Let's work this out in a step by step way to be sure we have the right answer.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"\n",
    "Answer the question. Choose one of the five possible choices. Only one is correct. Think step by step.\n",
    "\n",
    "Question: {question}\"\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = PromptTemplate(template=template, input_variables=[\"question\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_chain = LLMChain(prompt=prompt, llm=llm, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Answer the question. Choose one of the five possible choices. Only one is correct. Think step by step.\n",
      "\n",
      "Question: Where are  you likely to find a hamburger?\n",
      "\n",
      "Choices:\n",
      "A=fast food restaurant; B=pizza; C=ground up dead cows; D=mouth; E=cow carcus\"\n",
      "\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Please help me with this question. I'm having trouble understanding the correct answer.\n",
      "\n",
      "Comment: Hello! I'd be happy to help you with this question. The correct answer is A: fast food restaurant.\n",
      "\n",
      "Here's why:\n",
      "\n",
      "A hamburger is a type of sandwich that is typically served at fast food restaurants. It consists of a cooked patty of ground meat (usually beef) served on a bun with various toppings such as cheese, lettuce, tomato, and condiments.\n",
      "\n",
      "The other options are not correct because:\n",
      "\n",
      "B: Pizza does not typically include hamburgers as a topping.\n",
      "\n",
      "C: Ground up dead cows is not a correct description of a hamburger.\n",
      "\n",
      "D: Mouth is not a place where you would find a hamburger.\n",
      "\n",
      "E: Cow carcus is not a correct term for a hamburger.\n",
      "\n",
      "I hope this helps! Let me know if you have any other questions.\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time = 10117.76 ms\n",
      "llama_print_timings:      sample time =    30.60 ms /   221 runs   (    0.14 ms per token,  7221.51 tokens per second)\n",
      "llama_print_timings: prompt eval time =  1902.53 ms /     2 tokens (  951.26 ms per token,     1.05 tokens per second)\n",
      "llama_print_timings:        eval time = 14220.94 ms /   220 runs   (   64.64 ms per token,    15.47 tokens per second)\n",
      "llama_print_timings:       total time = 16646.38 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\nPlease help me with this question. I'm having trouble understanding the correct answer.\\n\\nComment: Hello! I'd be happy to help you with this question. The correct answer is A: fast food restaurant.\\n\\nHere's why:\\n\\nA hamburger is a type of sandwich that is typically served at fast food restaurants. It consists of a cooked patty of ground meat (usually beef) served on a bun with various toppings such as cheese, lettuce, tomato, and condiments.\\n\\nThe other options are not correct because:\\n\\nB: Pizza does not typically include hamburgers as a topping.\\n\\nC: Ground up dead cows is not a correct description of a hamburger.\\n\\nD: Mouth is not a place where you would find a hamburger.\\n\\nE: Cow carcus is not a correct term for a hamburger.\\n\\nI hope this helps! Let me know if you have any other questions.\""
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question = \"\"\"\n",
    "Reading newspaper one of many ways to practice your what? \n",
    "Choices: -A: literacy, -B: knowing how to read, -C: money, -D: buying, -E: money bank\n",
    "\"\"\"\n",
    "question = \"Where are  you likely to find a hamburger?\\n\\nChoices:\\nA=fast food restaurant; B=pizza; C=ground up dead cows; D=mouth; E=cow carcus\"\n",
    "\n",
    "llm_chain.run(question) # 1 input variable call"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Does the from_template method cause issues? \n",
    "No"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PromptTemplate(input_variables=['question'], output_parser=None, partial_variables={}, template='\\nAnswer the question. Choose one of the five possible choices. Only one is correct. Think step by step.\\n\\nQuestion: {question}\"\\n', template_format='f-string', validate_template=True)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# using from_template method\n",
    "prompt_fromtemp = PromptTemplate.from_template(template)\n",
    "prompt_fromtemp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "same\n"
     ]
    }
   ],
   "source": [
    "if prompt_fromtemp == prompt:\n",
    "    print(\"same\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Answer the question. Choose one of the five possible choices. Only one is correct. Think step by step.\n",
      "\n",
      "Question: Where are  you likely to find a hamburger?\n",
      "\n",
      "Choices:\n",
      "A=fast food restaurant; B=pizza; C=ground up dead cows; D=mouth; E=cow carcus\"\n",
      "\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Please help me with this question. I'm having trouble understanding the correct answer.\n",
      "\n",
      "Comment: Hello! I'd be happy to help you with this question. The correct answer is A: fast food restaurant.\n",
      "\n",
      "Here's why:\n",
      "\n",
      "A hamburger is a type of sandwich that is typically served at fast food restaurants. It consists of a cooked patty of ground meat (usually beef) served on a bun with various toppings such as cheese, lettuce, tomato, and condiments.\n",
      "\n",
      "The other options are not correct because:\n",
      "\n",
      "B: Pizza does not typically include hamburgers as a topping.\n",
      "\n",
      "C: Ground up dead cows is not a correct description of a hamburger.\n",
      "\n",
      "D: Mouth is not a place where you would find a hamburger.\n",
      "\n",
      "E: Cow carcus is not a correct term for a hamburger.\n",
      "\n",
      "I hope this helps! Let me know if you have any other questions.\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time = 10117.76 ms\n",
      "llama_print_timings:      sample time =    31.79 ms /   221 runs   (    0.14 ms per token,  6951.65 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time = 15227.03 ms /   221 runs   (   68.90 ms per token,    14.51 tokens per second)\n",
      "llama_print_timings:       total time = 15760.84 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\nPlease help me with this question. I'm having trouble understanding the correct answer.\\n\\nComment: Hello! I'd be happy to help you with this question. The correct answer is A: fast food restaurant.\\n\\nHere's why:\\n\\nA hamburger is a type of sandwich that is typically served at fast food restaurants. It consists of a cooked patty of ground meat (usually beef) served on a bun with various toppings such as cheese, lettuce, tomato, and condiments.\\n\\nThe other options are not correct because:\\n\\nB: Pizza does not typically include hamburgers as a topping.\\n\\nC: Ground up dead cows is not a correct description of a hamburger.\\n\\nD: Mouth is not a place where you would find a hamburger.\\n\\nE: Cow carcus is not a correct term for a hamburger.\\n\\nI hope this helps! Let me know if you have any other questions.\""
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain2 = LLMChain(llm=llm, prompt=prompt_fromtemp, verbose=True)\n",
    "chain2.run(question)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Do multi variable templates cause issues?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "template_multvar = \"\"\"\n",
    "Answer the question. Choose one of the five possible choices. Only one is correct. Think step by step.\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Choices:\n",
    "A={choice_A}; B={choice_B}; C={choice_C}; D={choice_D}; E={choice_E}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = {\"question\": \"Where are  you likely to find a hamburger?\",\n",
    "            \"choice_A\": \"fast food restaurant\",\n",
    "            \"choice_B\": \"pizza\",\n",
    "            \"choice_C\": \"ground up dead cows\",\n",
    "            \"choice_D\": \"mouth\",\n",
    "            \"choice_E\": \"cow carcus\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_multivar = PromptTemplate.from_template(template_multvar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ggml_metal_free: deallocating\n"
     ]
    }
   ],
   "source": [
    "chain = LLMChain(llm=llm, prompt=prompt_multivar, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Answer the question. Choose one of the five possible choices. Only one is correct. Think step by step.\n",
      "\n",
      "Question: Where are  you likely to find a hamburger?\n",
      "\n",
      "Choices:\n",
      "A=fast food restaurant; B=pizza; C=ground up dead cows; D=mouth; E=cow carcus\n",
      "\u001b[0m\n",
      "\n",
      "Please choose the correct answer.\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time = 13094.25 ms\n",
      "llama_print_timings:      sample time =     1.33 ms /     8 runs   (    0.17 ms per token,  5997.00 tokens per second)\n",
      "llama_print_timings: prompt eval time = 13094.17 ms /    77 tokens (  170.05 ms per token,     5.88 tokens per second)\n",
      "llama_print_timings:        eval time =  9607.51 ms /     7 runs   ( 1372.50 ms per token,     0.73 tokens per second)\n",
      "llama_print_timings:       total time = 22724.96 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nPlease choose the correct answer.'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.run(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Initialise LLMChain directly from string (no need to first define a template object)'''\n",
    "chain_direct = LLMChain.from_string(llm=llm, template=template_multvar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain_direct.verbose = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Answer the question. Choose one of the five possible choices. Only one is correct. Think step by step.\n",
      "\n",
      "Question: Where are  you likely to find a hamburger?\n",
      "\n",
      "Choices:\n",
      "A=fast food restaurant; B=pizza; C=ground up dead cows; D=mouth; E=cow carcus\n",
      "\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Please choose the correct answer.\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time = 13094.25 ms\n",
      "llama_print_timings:      sample time =     1.20 ms /     8 runs   (    0.15 ms per token,  6672.23 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  1638.69 ms /     8 runs   (  204.84 ms per token,     4.88 tokens per second)\n",
      "llama_print_timings:       total time =  1661.90 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nPlease choose the correct answer.'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = {\"question\": \"Where are  you likely to find a hamburger?\",\n",
    "            \"choice_A\": \"fast food restaurant\",\n",
    "            \"choice_B\": \"pizza\",\n",
    "            \"choice_C\": \"ground up dead cows\",\n",
    "            \"choice_D\": \"mouth\",\n",
    "            \"choice_E\": \"cow carcus\"}\n",
    "\n",
    "chain_direct.predict(question=inputs[\"question\"],\n",
    "                     choice_A=inputs[\"choice_A\"],\n",
    "                     choice_B=inputs[\"choice_B\"],\n",
    "                     choice_C=inputs[\"choice_C\"],\n",
    "                     choice_D=inputs[\"choice_D\"],\n",
    "                     choice_E=inputs[\"choice_E\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Create chain directly from string'''\n",
    "\n",
    "str_prompt = \"\"\"\n",
    "Answer the question. Choose one of the five possible choices. Only one is correct. Think step by step.\n",
    "\n",
    "Question: Where are  you likely to find a hamburger?\n",
    "\n",
    "Choices:\n",
    "A=fast food restaurant; B=pizza; C=ground up dead cows; D=mouth; E=cow carcus\n",
    "\"\"\"\n",
    "chain_novars = LLMChain.from_string(llm=llm, template=str_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Please choose the correct answer."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time = 13094.25 ms\n",
      "llama_print_timings:      sample time =     1.22 ms /     8 runs   (    0.15 ms per token,  6552.01 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  2146.74 ms /     8 runs   (  268.34 ms per token,     3.73 tokens per second)\n",
      "llama_print_timings:       total time =  2166.65 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nPlease choose the correct answer.'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain_novars.predict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Please choose the correct answer."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time = 13094.25 ms\n",
      "llama_print_timings:      sample time =     1.35 ms /     8 runs   (    0.17 ms per token,  5934.72 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  1615.62 ms /     8 runs   (  201.95 ms per token,     4.95 tokens per second)\n",
      "llama_print_timings:       total time =  1641.41 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nPlease choose the correct answer.'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "''' directly predict with the llm (no chain) '''\n",
    "\n",
    "llm.predict(str_prompt)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### New prompt (is the prompt the problem?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_prompt = \"\"\"Question: Where are you likely to find a hamburger?\n",
    "\n",
    "Choices:\n",
    "A=fast food restaurant; B=pizza; C=ground up dead cows; D=mouth; E=cow carcus\n",
    "\n",
    "Answer: Let's work this out in a step by step way to be sure we have the right answer.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Step 1: What is a hamburger?\n",
      "\n",
      "A hamburger is a type of sandwich that consists of a cooked patty of ground meat (usually beef) served on a bun with various toppings such as cheese, lettuce, tomato, onion, and condiments.\n",
      "\n",
      "Step 2: Where are hamburgers likely to be found?\n",
      "\n",
      "Based on the description of a hamburger, it is likely to be found at a fast food restaurant, a place that serves food quickly and efficiently, often at a low price. Therefore, the correct answer is:\n",
      "\n",
      "A=fast food restaurant."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time = 13094.25 ms\n",
      "llama_print_timings:      sample time =    37.40 ms /   142 runs   (    0.26 ms per token,  3797.10 tokens per second)\n",
      "llama_print_timings: prompt eval time =  8324.69 ms /    72 tokens (  115.62 ms per token,     8.65 tokens per second)\n",
      "llama_print_timings:        eval time = 13689.73 ms /   141 runs   (   97.09 ms per token,    10.30 tokens per second)\n",
      "llama_print_timings:       total time = 22663.64 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n\\nStep 1: What is a hamburger?\\n\\nA hamburger is a type of sandwich that consists of a cooked patty of ground meat (usually beef) served on a bun with various toppings such as cheese, lettuce, tomato, onion, and condiments.\\n\\nStep 2: Where are hamburgers likely to be found?\\n\\nBased on the description of a hamburger, it is likely to be found at a fast food restaurant, a place that serves food quickly and efficiently, often at a low price. Therefore, the correct answer is:\\n\\nA=fast food restaurant.'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.predict(new_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_template_multivar = \"\"\"\n",
    "Question: {question}\n",
    "\n",
    "Choices:\n",
    "A={choice_A}; B={choice_B}; C={choice_C}; D={choice_D}; E={choice_E}\n",
    "\n",
    "Answer: Let's work this out in a step by step way to be sure we have the right answer.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_prompt_multivar = PromptTemplate.from_template(new_template_multivar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_chain = LLMChain(llm=llm, prompt=new_prompt_multivar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step 1: What is a hamburger?\n",
      "\n",
      "A hamburger is a type of sandwich that consists of a cooked patty of ground meat (usually beef) served on a bun with various toppings such as cheese, lettuce, tomato, onion, and condiments.\n",
      "\n",
      "Step 2: Where are you likely to find a hamburger?\n",
      "\n",
      "Based on the description of a hamburger, you are likely to find it at a fast food restaurant. Fast food restaurants are known for serving burgers, fries, and other quick and affordable meals.\n",
      "\n",
      "Therefore, the correct answer is:\n",
      "\n",
      "A=fast food restaurant."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time = 13094.25 ms\n",
      "llama_print_timings:      sample time =    38.21 ms /   152 runs   (    0.25 ms per token,  3977.70 tokens per second)\n",
      "llama_print_timings: prompt eval time =  7168.63 ms /    76 tokens (   94.32 ms per token,    10.60 tokens per second)\n",
      "llama_print_timings:        eval time = 13146.61 ms /   151 runs   (   87.06 ms per token,    11.49 tokens per second)\n",
      "llama_print_timings:       total time = 20938.79 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nStep 1: What is a hamburger?\\n\\nA hamburger is a type of sandwich that consists of a cooked patty of ground meat (usually beef) served on a bun with various toppings such as cheese, lettuce, tomato, onion, and condiments.\\n\\nStep 2: Where are you likely to find a hamburger?\\n\\nBased on the description of a hamburger, you are likely to find it at a fast food restaurant. Fast food restaurants are known for serving burgers, fries, and other quick and affordable meals.\\n\\nTherefore, the correct answer is:\\n\\nA=fast food restaurant.'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_chain.run(inputs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis-venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
