{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Documentation for call methods\n",
    "\n",
    "https://api.python.langchain.com/en/latest/chains/langchain.chains.llm.LLMChain.html#langchain.chains.llm.\n",
    "\n",
    "https://python.langchain.com/docs/modules/chains/how_to/call_methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "\n",
    "from dotenv import load_dotenv, find_dotenv, dotenv_values\n",
    "\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "\n",
    "import utils.data\n",
    "import utils.models\n",
    "import utils.prompting\n",
    "\n",
    "import pprint as pp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = load_dotenv(find_dotenv())\n",
    "if _ is not True:\n",
    "    raise Exception(\"No .env file found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:There are multiple 'dev' files; using .gz file\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = utils.data.load_data()\n",
    "data = utils.data.flatten_CoQA(data)\n",
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = utils.prompting.create_template(\n",
    "    prompting_type='baseline_SBS',\n",
    "    dataset='commonsenseQA',\n",
    "    output_formatting=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "llm = ChatOpenAI(temperature = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = utils.models.create_LLMchain(llm, prompt_template, verbose=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use LLMChain; format prompt during the call\n",
    "+ current call in main.py\n",
    "+ only the first instance gets an explanation, the others just the answer (!! looks like it's not just the firs that gets an explanation, but other ones as well when running on more instances..)\n",
    "+ prompt formatting is okay (all prompts contains 'lets think sbs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mHuman: Answer the following question. There are 5 possible answer and only 1 is correct. \n",
      "\n",
      "Question: A revolving door is convenient for two direction travel, but it also serves as a security measure at a what? \n",
      "\n",
      "- A: bank \n",
      "- B: library \n",
      "- C: department store \n",
      "- D: mall \n",
      "- E: new york \n",
      "\n",
      "Choose the correct answer. Let's think step by step.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mHuman: Answer the following question. There are 5 possible answer and only 1 is correct. \n",
      "\n",
      "Question: What do people aim to do at work? \n",
      "\n",
      "- A: complete job \n",
      "- B: learn from each other \n",
      "- C: kill animals \n",
      "- D: wear hats \n",
      "- E: talk to each other \n",
      "\n",
      "Choose the correct answer. Let's think step by step.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mHuman: Answer the following question. There are 5 possible answer and only 1 is correct. \n",
      "\n",
      "Question: Where would you find magazines along side many other printed works? \n",
      "\n",
      "- A: doctor \n",
      "- B: bookstore \n",
      "- C: market \n",
      "- D: train station \n",
      "- E: mortuary \n",
      "\n",
      "Choose the correct answer. Let's think step by step.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Responses:\n",
      "{0: 'The question asks about a revolving door serving as a security measure. '\n",
      "    'To determine the correct answer, we need to consider which location would '\n",
      "    'benefit from having a security measure like a revolving door.\\n'\n",
      "    '\\n'\n",
      "    'A: A bank is a possible option as it often has security measures in '\n",
      "    'place. A revolving door could help control access and prevent '\n",
      "    'unauthorized entry.\\n'\n",
      "    '\\n'\n",
      "    'B: A library may not require a revolving door as a security measure. It '\n",
      "    'is more likely to focus on other security measures such as surveillance '\n",
      "    'cameras or alarms.\\n'\n",
      "    '\\n'\n",
      "    'C: A department store could benefit from a revolving door as a security '\n",
      "    'measure to control the flow of customers and deter theft.\\n'\n",
      "    '\\n'\n",
      "    'D: A mall is another possible option as it often has multiple entrances '\n",
      "    'and exits. A revolving door could help regulate the flow of people and '\n",
      "    'enhance security.\\n'\n",
      "    '\\n'\n",
      "    'E: \"New York\" is not a specific location but a city. It does not fit the '\n",
      "    'context of the question.\\n'\n",
      "    '\\n'\n",
      "    'Based on the analysis, the most suitable answer is either A: bank or D: '\n",
      "    'mall.',\n",
      " 1: 'The correct answer is B: learn from each other.',\n",
      " 2: 'The correct answer is B: bookstore. Bookstores typically have a wide '\n",
      "    'range of printed works, including magazines, available for purchase.'}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pprint as pp\n",
    "\n",
    "responses = {}\n",
    "for idx, instance in enumerate(data):\n",
    "    response = chain(\n",
    "        return_only_outputs=True, \n",
    "        inputs={\n",
    "            'question':instance[\"stem\"],\n",
    "            'choice_A':instance[\"choice_A\"],\n",
    "            'choice_B':instance[\"choice_B\"],\n",
    "            'choice_C':instance[\"choice_C\"],\n",
    "            'choice_D':instance[\"choice_D\"],\n",
    "            'choice_E':instance[\"choice_E\"]\n",
    "            }\n",
    "        )\n",
    "    responses[idx] = response['text']\n",
    "\n",
    "print(f\"Responses:\\n{pp.pformat(responses)}\\n\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## No Chain; format prompt before the call\n",
    "+ same problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[HumanMessage(content=\"Answer the following question. There are 5 possible answer and only 1 is correct. \\n\\nQuestion: A revolving door is convenient for two direction travel, but it also serves as a security measure at a what? \\n\\n- A: bank \\n- B: library \\n- C: department store \\n- D: mall \\n- E: new york \\n\\nChoose the correct answer. Let's think step by step.\", additional_kwargs={}, example=False)],\n",
       " [HumanMessage(content=\"Answer the following question. There are 5 possible answer and only 1 is correct. \\n\\nQuestion: What do people aim to do at work? \\n\\n- A: complete job \\n- B: learn from each other \\n- C: kill animals \\n- D: wear hats \\n- E: talk to each other \\n\\nChoose the correct answer. Let's think step by step.\", additional_kwargs={}, example=False)],\n",
       " [HumanMessage(content=\"Answer the following question. There are 5 possible answer and only 1 is correct. \\n\\nQuestion: Where would you find magazines along side many other printed works? \\n\\n- A: doctor \\n- B: bookstore \\n- C: market \\n- D: train station \\n- E: mortuary \\n\\nChoose the correct answer. Let's think step by step.\", additional_kwargs={}, example=False)],\n",
       " [HumanMessage(content=\"Answer the following question. There are 5 possible answer and only 1 is correct. \\n\\nQuestion: Where are  you likely to find a hamburger? \\n\\n- A: fast food restaurant \\n- B: pizza \\n- C: ground up dead cows \\n- D: mouth \\n- E: cow carcus \\n\\nChoose the correct answer. Let's think step by step.\", additional_kwargs={}, example=False)],\n",
       " [HumanMessage(content=\"Answer the following question. There are 5 possible answer and only 1 is correct. \\n\\nQuestion: James was looking for a good place to buy farmland.  Where might he look? \\n\\n- A: midwest \\n- B: countryside \\n- C: estate \\n- D: farming areas \\n- E: illinois \\n\\nChoose the correct answer. Let's think step by step.\", additional_kwargs={}, example=False)],\n",
       " [HumanMessage(content=\"Answer the following question. There are 5 possible answer and only 1 is correct. \\n\\nQuestion: What island country is ferret popular? \\n\\n- A: own home \\n- B: north carolina \\n- C: great britain \\n- D: hutch \\n- E: outdoors \\n\\nChoose the correct answer. Let's think step by step.\", additional_kwargs={}, example=False)],\n",
       " [HumanMessage(content=\"Answer the following question. There are 5 possible answer and only 1 is correct. \\n\\nQuestion: In what Spanish speaking North American country can you get a great cup of coffee? \\n\\n- A: mildred's coffee shop \\n- B: mexico \\n- C: diner \\n- D: kitchen \\n- E: canteen \\n\\nChoose the correct answer. Let's think step by step.\", additional_kwargs={}, example=False)],\n",
       " [HumanMessage(content=\"Answer the following question. There are 5 possible answer and only 1 is correct. \\n\\nQuestion: What do animals do when an enemy is approaching? \\n\\n- A: feel pleasure \\n- B: procreate \\n- C: pass water \\n- D: listen to each other \\n- E: sing \\n\\nChoose the correct answer. Let's think step by step.\", additional_kwargs={}, example=False)],\n",
       " [HumanMessage(content=\"Answer the following question. There are 5 possible answer and only 1 is correct. \\n\\nQuestion: Reading newspaper one of many ways to practice your what? \\n\\n- A: literacy \\n- B: knowing how to read \\n- C: money \\n- D: buying \\n- E: money bank \\n\\nChoose the correct answer. Let's think step by step.\", additional_kwargs={}, example=False)],\n",
       " [HumanMessage(content=\"Answer the following question. There are 5 possible answer and only 1 is correct. \\n\\nQuestion: What do people typically do while playing guitar? \\n\\n- A: cry \\n- B: hear sounds \\n- C: singing \\n- D: arthritis \\n- E: making music \\n\\nChoose the correct answer. Let's think step by step.\", additional_kwargs={}, example=False)]]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompts = []\n",
    "\n",
    "for instance in data:\n",
    "    prompt_lc = prompt_template.format_messages(\n",
    "        question=instance[\"stem\"],\n",
    "        choice_A=instance[\"choice_A\"],\n",
    "        choice_B=instance[\"choice_B\"],\n",
    "        choice_C=instance[\"choice_C\"],\n",
    "        choice_D=instance[\"choice_D\"],\n",
    "        choice_E=instance[\"choice_E\"]\n",
    "    )\n",
    "    prompts.append(prompt_lc)\n",
    "\n",
    "prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Responses:\n",
      "{0: 'The question asks about a revolving door serving as a security measure. '\n",
      "    'To determine the correct answer, we need to consider which location would '\n",
      "    'benefit from having a security measure like a revolving door.\\n'\n",
      "    '\\n'\n",
      "    'A: A bank is a possible option as it often has security measures in '\n",
      "    'place. A revolving door could help control access and prevent '\n",
      "    'unauthorized entry.\\n'\n",
      "    '\\n'\n",
      "    'B: A library may not require a revolving door as a security measure. It '\n",
      "    'is more likely to focus on other security measures such as surveillance '\n",
      "    'cameras or alarms.\\n'\n",
      "    '\\n'\n",
      "    'C: A department store could benefit from a revolving door as a security '\n",
      "    'measure to control the flow of customers and deter theft.\\n'\n",
      "    '\\n'\n",
      "    'D: A mall is another possible option as it often has multiple entrances '\n",
      "    'and exits. A revolving door could help regulate the flow of people and '\n",
      "    'enhance security.\\n'\n",
      "    '\\n'\n",
      "    'E: \"New York\" is not a specific location but a city. It does not fit the '\n",
      "    'context of the question.\\n'\n",
      "    '\\n'\n",
      "    'Based on the analysis, the most suitable answer is either A: bank or D: '\n",
      "    'mall.',\n",
      " 1: 'The correct answer is B: learn from each other.',\n",
      " 2: 'The correct answer is B: bookstore. Bookstores typically have a wide '\n",
      "    'range of printed works, including magazines, available for purchase.',\n",
      " 3: 'The correct answer is A: fast food restaurant.',\n",
      " 4: 'The correct answer is A: midwest. \\n'\n",
      "    '\\n'\n",
      "    'Step 1: James is looking for farmland, which suggests he is interested in '\n",
      "    'agricultural areas.\\n'\n",
      "    'Step 2: The options B, C, and E are more specific locations within a '\n",
      "    'larger region, so they can be eliminated.\\n'\n",
      "    'Step 3: The remaining option, D: farming areas, is too broad and does not '\n",
      "    'provide a specific location.\\n'\n",
      "    'Step 4: Therefore, the best choice is A: midwest, as it is a well-known '\n",
      "    'region in the United States for its agricultural activities and farmland '\n",
      "    'availability.',\n",
      " 5: 'To determine the correct answer, we need to consider which island country '\n",
      "    'ferrets are popular in. \\n'\n",
      "    '\\n'\n",
      "    '- A: \"own home\" does not refer to an island country.\\n'\n",
      "    '- B: \"north carolina\" is not an island country.\\n'\n",
      "    '- C: \"great britain\" is an island country, but it does not specify if '\n",
      "    'ferrets are popular there.\\n'\n",
      "    '- D: \"hutch\" does not refer to an island country.\\n'\n",
      "    '- E: \"outdoors\" does not refer to an island country.\\n'\n",
      "    '\\n'\n",
      "    'Based on the options provided, none of them explicitly state an island '\n",
      "    'country where ferrets are popular. Therefore, none of the given answers '\n",
      "    'are correct.',\n",
      " 6: 'The correct answer is B: Mexico.',\n",
      " 7: 'The correct answer is D: listen to each other.',\n",
      " 8: 'The correct answer is A: literacy. Reading newspaper is one of the many '\n",
      "    'ways to practice your literacy skills, as it helps improve reading '\n",
      "    'comprehension, vocabulary, and overall understanding of written texts.',\n",
      " 9: 'The correct answer is E: making music.'}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "responses = {}\n",
    "for idx, prompt in enumerate(prompts):\n",
    "    response = llm(prompt)\n",
    "    responses[idx] = response.content\n",
    "\n",
    "print(f\"Responses:\\n{pp.pformat(responses)}\\n\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Isolated calls (no loop)\n",
    "+ same problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The correct answer is B: learn from each other.'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = llm(prompts[1])\n",
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The correct answer is B: bookstore. Bookstores typically have a wide range of printed works, including magazines, available for purchase.'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = llm(prompts[2])\n",
    "response.content"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Format prompt for Chat model (sys message, human message separate)\n",
    "+ formatting the prompt according to chat models guidelines does not solve the problem. The generation is exactly the same.\n",
    "+ !! changed the sys prompt and solved the issue. Therefore the issue is with the prompt (let's think sbs is not enough to generate explanations with this model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import (\n",
    "    ChatPromptTemplate,\n",
    "    PromptTemplate,\n",
    "    SystemMessagePromptTemplate,\n",
    "    AIMessagePromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    ")\n",
    "from langchain.schema import (\n",
    "    AIMessage,\n",
    "    HumanMessage,\n",
    "    SystemMessage\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys_message_prompt = SystemMessagePromptTemplate.from_template(\n",
    "    \"Your task is to answer the question. There are 5 possible answer and only one is correct. Always choose exactly one answer. Always explain your answer. Think step by step.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "human_message_prompt = HumanMessagePromptTemplate.from_template(\n",
    "    \"Question: {question}\\n\\nChoices:\\n-A: {choice_A}\\n-B: {choice_B}\\n-C: {choice_C}\\n-D: {choice_D}\\n-E: {choice_E}\\n\\n\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_prompt_template = ChatPromptTemplate.from_messages([sys_message_prompt, human_message_prompt])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['choice_A', 'choice_B', 'choice_C', 'choice_D', 'choice_E', 'question'], output_parser=None, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], output_parser=None, partial_variables={}, template='Your task is to answer the question. There are 5 possible answer and only one is correct. Always choose exactly one answer. Always explain your answer. Think step by step.', template_format='f-string', validate_template=True), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['choice_A', 'choice_B', 'choice_C', 'choice_D', 'choice_E', 'question'], output_parser=None, partial_variables={}, template='Question: {question}\\n\\nChoices:\\n-A: {choice_A}\\n-B: {choice_B}\\n-C: {choice_C}\\n-D: {choice_D}\\n-E: {choice_E}\\n\\n', template_format='f-string', validate_template=True), additional_kwargs={})])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_prompt_template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[SystemMessage(content='Your task is to answer the question. There are 5 possible answer and only one is correct. Always choose exactly one answer. Always explain your answer. Think step by step.', additional_kwargs={}),\n",
       "  HumanMessage(content='Question: A revolving door is convenient for two direction travel, but it also serves as a security measure at a what?\\n\\nChoices:\\n-A: bank\\n-B: library\\n-C: department store\\n-D: mall\\n-E: new york\\n\\n', additional_kwargs={}, example=False)],\n",
       " [SystemMessage(content='Your task is to answer the question. There are 5 possible answer and only one is correct. Always choose exactly one answer. Always explain your answer. Think step by step.', additional_kwargs={}),\n",
       "  HumanMessage(content='Question: What do people aim to do at work?\\n\\nChoices:\\n-A: complete job\\n-B: learn from each other\\n-C: kill animals\\n-D: wear hats\\n-E: talk to each other\\n\\n', additional_kwargs={}, example=False)],\n",
       " [SystemMessage(content='Your task is to answer the question. There are 5 possible answer and only one is correct. Always choose exactly one answer. Always explain your answer. Think step by step.', additional_kwargs={}),\n",
       "  HumanMessage(content='Question: Where would you find magazines along side many other printed works?\\n\\nChoices:\\n-A: doctor\\n-B: bookstore\\n-C: market\\n-D: train station\\n-E: mortuary\\n\\n', additional_kwargs={}, example=False)],\n",
       " [SystemMessage(content='Your task is to answer the question. There are 5 possible answer and only one is correct. Always choose exactly one answer. Always explain your answer. Think step by step.', additional_kwargs={}),\n",
       "  HumanMessage(content='Question: Where are  you likely to find a hamburger?\\n\\nChoices:\\n-A: fast food restaurant\\n-B: pizza\\n-C: ground up dead cows\\n-D: mouth\\n-E: cow carcus\\n\\n', additional_kwargs={}, example=False)],\n",
       " [SystemMessage(content='Your task is to answer the question. There are 5 possible answer and only one is correct. Always choose exactly one answer. Always explain your answer. Think step by step.', additional_kwargs={}),\n",
       "  HumanMessage(content='Question: James was looking for a good place to buy farmland.  Where might he look?\\n\\nChoices:\\n-A: midwest\\n-B: countryside\\n-C: estate\\n-D: farming areas\\n-E: illinois\\n\\n', additional_kwargs={}, example=False)],\n",
       " [SystemMessage(content='Your task is to answer the question. There are 5 possible answer and only one is correct. Always choose exactly one answer. Always explain your answer. Think step by step.', additional_kwargs={}),\n",
       "  HumanMessage(content='Question: What island country is ferret popular?\\n\\nChoices:\\n-A: own home\\n-B: north carolina\\n-C: great britain\\n-D: hutch\\n-E: outdoors\\n\\n', additional_kwargs={}, example=False)],\n",
       " [SystemMessage(content='Your task is to answer the question. There are 5 possible answer and only one is correct. Always choose exactly one answer. Always explain your answer. Think step by step.', additional_kwargs={}),\n",
       "  HumanMessage(content=\"Question: In what Spanish speaking North American country can you get a great cup of coffee?\\n\\nChoices:\\n-A: mildred's coffee shop\\n-B: mexico\\n-C: diner\\n-D: kitchen\\n-E: canteen\\n\\n\", additional_kwargs={}, example=False)],\n",
       " [SystemMessage(content='Your task is to answer the question. There are 5 possible answer and only one is correct. Always choose exactly one answer. Always explain your answer. Think step by step.', additional_kwargs={}),\n",
       "  HumanMessage(content='Question: What do animals do when an enemy is approaching?\\n\\nChoices:\\n-A: feel pleasure\\n-B: procreate\\n-C: pass water\\n-D: listen to each other\\n-E: sing\\n\\n', additional_kwargs={}, example=False)],\n",
       " [SystemMessage(content='Your task is to answer the question. There are 5 possible answer and only one is correct. Always choose exactly one answer. Always explain your answer. Think step by step.', additional_kwargs={}),\n",
       "  HumanMessage(content='Question: Reading newspaper one of many ways to practice your what?\\n\\nChoices:\\n-A: literacy\\n-B: knowing how to read\\n-C: money\\n-D: buying\\n-E: money bank\\n\\n', additional_kwargs={}, example=False)],\n",
       " [SystemMessage(content='Your task is to answer the question. There are 5 possible answer and only one is correct. Always choose exactly one answer. Always explain your answer. Think step by step.', additional_kwargs={}),\n",
       "  HumanMessage(content='Question: What do people typically do while playing guitar?\\n\\nChoices:\\n-A: cry\\n-B: hear sounds\\n-C: singing\\n-D: arthritis\\n-E: making music\\n\\n', additional_kwargs={}, example=False)]]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_prompts = []\n",
    "for instance in data:\n",
    "    chat_prompt = chat_prompt_template.format_prompt(\n",
    "        question=instance[\"stem\"],\n",
    "        choice_A=instance[\"choice_A\"],\n",
    "        choice_B=instance[\"choice_B\"],\n",
    "        choice_C=instance[\"choice_C\"],\n",
    "        choice_D=instance[\"choice_D\"],\n",
    "        choice_E=instance[\"choice_E\"]\n",
    "    ).to_messages()\n",
    "    chat_prompts.append(chat_prompt)\n",
    "\n",
    "chat_prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"The correct answer is A: complete job.\\n\\nWhen people go to work, their primary aim is to complete the tasks and responsibilities assigned to them. This involves performing their job duties, meeting deadlines, and achieving the goals set by their employers. The purpose of work is to contribute to the success of the organization or company they work for, whether it's through producing goods, providing services, or fulfilling specific roles within the company's structure. While learning from each other, wearing hats, and talking to each other may occur in a work environment, they are not the primary objectives of work. Killing animals is not a typical aim of work, unless someone is specifically employed in a profession related to hunting or animal control.\", additional_kwargs={}, example=False)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = llm(chat_prompts[1])\n",
    "response"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Change prompt so that it's only the question + let's think sbs\n",
    "### + output formatting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "template_str = \"\"\"\n",
    "Question: {question}\\n\\n\n",
    "Choices:\\n-A: {choice_A}\\n-B: {choice_B}\\n-C: {choice_C}\\n-D: {choice_D}\\n-E: {choice_E}\\n\\n\n",
    "Let's think step by step.\\n\\n\n",
    "{format_instructions}\n",
    "\"\"\"\n",
    "\n",
    "new_prompt_template = ChatPromptTemplate.from_template(template_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.output_parsers import StructuredOutputParser, ResponseSchema\n",
    "\n",
    "prediction_letter_schema = ResponseSchema(\n",
    "    name=\"prediction_letter\", \n",
    "    description=\"This is the answer to the question. It is one letter (either A, B, C, D, or E), which corresponds to the answer chosen by the model.\"\n",
    ")\n",
    "\n",
    "prediction_text_schema = ResponseSchema(\n",
    "    name=\"prediction_text\", \n",
    "    description=\"This is the answer to the question. It is the text corresponding to the letter chosen as the answer. It is usually one word or a short phrase.\"\n",
    ")\n",
    "\n",
    "explanation_schema = ResponseSchema(\n",
    "    name=\"explanation\", \n",
    "    description=\"This is the explanation for the prediction. It is a piece of text explains why the model chose the answer it did.\"\n",
    ")\n",
    "\n",
    "schemas = [prediction_letter_schema, prediction_text_schema, explanation_schema]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_parser = StructuredOutputParser.from_response_schemas(schemas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n```json\\n{\\n\\t\"prediction_letter\": string  // This is the answer to the question. It is one letter (either A, B, C, D, or E), which corresponds to the answer chosen by the model.\\n\\t\"prediction_text\": string  // This is the answer to the question. It is the text corresponding to the letter chosen as the answer. It is usually one word or a short phrase.\\n\\t\"explanation\": string  // This is the explanation for the prediction. It is a piece of text explains why the model chose the answer it did.\\n}\\n'"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "format_instructions = output_parser.get_format_instructions(only_json=True)\n",
    "format_instructions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[HumanMessage(content='\\nQuestion: A revolving door is convenient for two direction travel, but it also serves as a security measure at a what?\\n\\n\\nChoices:\\n-A: bank\\n-B: library\\n-C: department store\\n-D: mall\\n-E: new york\\n\\n\\nLet\\'s think step by step.\\n\\n\\n\\n```json\\n{\\n\\t\"prediction_letter\": string  // This is the answer to the question. It is one letter (either A, B, C, D, or E), which corresponds to the answer chosen by the model.\\n\\t\"prediction_text\": string  // This is the answer to the question. It is the text corresponding to the letter chosen as the answer. It is usually one word or a short phrase.\\n\\t\"explanation\": string  // This is the explanation for the prediction. It is a piece of text explains why the model chose the answer it did.\\n}\\n\\n', additional_kwargs={}, example=False)],\n",
       " [HumanMessage(content='\\nQuestion: What do people aim to do at work?\\n\\n\\nChoices:\\n-A: complete job\\n-B: learn from each other\\n-C: kill animals\\n-D: wear hats\\n-E: talk to each other\\n\\n\\nLet\\'s think step by step.\\n\\n\\n\\n```json\\n{\\n\\t\"prediction_letter\": string  // This is the answer to the question. It is one letter (either A, B, C, D, or E), which corresponds to the answer chosen by the model.\\n\\t\"prediction_text\": string  // This is the answer to the question. It is the text corresponding to the letter chosen as the answer. It is usually one word or a short phrase.\\n\\t\"explanation\": string  // This is the explanation for the prediction. It is a piece of text explains why the model chose the answer it did.\\n}\\n\\n', additional_kwargs={}, example=False)],\n",
       " [HumanMessage(content='\\nQuestion: Where would you find magazines along side many other printed works?\\n\\n\\nChoices:\\n-A: doctor\\n-B: bookstore\\n-C: market\\n-D: train station\\n-E: mortuary\\n\\n\\nLet\\'s think step by step.\\n\\n\\n\\n```json\\n{\\n\\t\"prediction_letter\": string  // This is the answer to the question. It is one letter (either A, B, C, D, or E), which corresponds to the answer chosen by the model.\\n\\t\"prediction_text\": string  // This is the answer to the question. It is the text corresponding to the letter chosen as the answer. It is usually one word or a short phrase.\\n\\t\"explanation\": string  // This is the explanation for the prediction. It is a piece of text explains why the model chose the answer it did.\\n}\\n\\n', additional_kwargs={}, example=False)],\n",
       " [HumanMessage(content='\\nQuestion: Where are  you likely to find a hamburger?\\n\\n\\nChoices:\\n-A: fast food restaurant\\n-B: pizza\\n-C: ground up dead cows\\n-D: mouth\\n-E: cow carcus\\n\\n\\nLet\\'s think step by step.\\n\\n\\n\\n```json\\n{\\n\\t\"prediction_letter\": string  // This is the answer to the question. It is one letter (either A, B, C, D, or E), which corresponds to the answer chosen by the model.\\n\\t\"prediction_text\": string  // This is the answer to the question. It is the text corresponding to the letter chosen as the answer. It is usually one word or a short phrase.\\n\\t\"explanation\": string  // This is the explanation for the prediction. It is a piece of text explains why the model chose the answer it did.\\n}\\n\\n', additional_kwargs={}, example=False)],\n",
       " [HumanMessage(content='\\nQuestion: James was looking for a good place to buy farmland.  Where might he look?\\n\\n\\nChoices:\\n-A: midwest\\n-B: countryside\\n-C: estate\\n-D: farming areas\\n-E: illinois\\n\\n\\nLet\\'s think step by step.\\n\\n\\n\\n```json\\n{\\n\\t\"prediction_letter\": string  // This is the answer to the question. It is one letter (either A, B, C, D, or E), which corresponds to the answer chosen by the model.\\n\\t\"prediction_text\": string  // This is the answer to the question. It is the text corresponding to the letter chosen as the answer. It is usually one word or a short phrase.\\n\\t\"explanation\": string  // This is the explanation for the prediction. It is a piece of text explains why the model chose the answer it did.\\n}\\n\\n', additional_kwargs={}, example=False)],\n",
       " [HumanMessage(content='\\nQuestion: What island country is ferret popular?\\n\\n\\nChoices:\\n-A: own home\\n-B: north carolina\\n-C: great britain\\n-D: hutch\\n-E: outdoors\\n\\n\\nLet\\'s think step by step.\\n\\n\\n\\n```json\\n{\\n\\t\"prediction_letter\": string  // This is the answer to the question. It is one letter (either A, B, C, D, or E), which corresponds to the answer chosen by the model.\\n\\t\"prediction_text\": string  // This is the answer to the question. It is the text corresponding to the letter chosen as the answer. It is usually one word or a short phrase.\\n\\t\"explanation\": string  // This is the explanation for the prediction. It is a piece of text explains why the model chose the answer it did.\\n}\\n\\n', additional_kwargs={}, example=False)],\n",
       " [HumanMessage(content='\\nQuestion: In what Spanish speaking North American country can you get a great cup of coffee?\\n\\n\\nChoices:\\n-A: mildred\\'s coffee shop\\n-B: mexico\\n-C: diner\\n-D: kitchen\\n-E: canteen\\n\\n\\nLet\\'s think step by step.\\n\\n\\n\\n```json\\n{\\n\\t\"prediction_letter\": string  // This is the answer to the question. It is one letter (either A, B, C, D, or E), which corresponds to the answer chosen by the model.\\n\\t\"prediction_text\": string  // This is the answer to the question. It is the text corresponding to the letter chosen as the answer. It is usually one word or a short phrase.\\n\\t\"explanation\": string  // This is the explanation for the prediction. It is a piece of text explains why the model chose the answer it did.\\n}\\n\\n', additional_kwargs={}, example=False)],\n",
       " [HumanMessage(content='\\nQuestion: What do animals do when an enemy is approaching?\\n\\n\\nChoices:\\n-A: feel pleasure\\n-B: procreate\\n-C: pass water\\n-D: listen to each other\\n-E: sing\\n\\n\\nLet\\'s think step by step.\\n\\n\\n\\n```json\\n{\\n\\t\"prediction_letter\": string  // This is the answer to the question. It is one letter (either A, B, C, D, or E), which corresponds to the answer chosen by the model.\\n\\t\"prediction_text\": string  // This is the answer to the question. It is the text corresponding to the letter chosen as the answer. It is usually one word or a short phrase.\\n\\t\"explanation\": string  // This is the explanation for the prediction. It is a piece of text explains why the model chose the answer it did.\\n}\\n\\n', additional_kwargs={}, example=False)],\n",
       " [HumanMessage(content='\\nQuestion: Reading newspaper one of many ways to practice your what?\\n\\n\\nChoices:\\n-A: literacy\\n-B: knowing how to read\\n-C: money\\n-D: buying\\n-E: money bank\\n\\n\\nLet\\'s think step by step.\\n\\n\\n\\n```json\\n{\\n\\t\"prediction_letter\": string  // This is the answer to the question. It is one letter (either A, B, C, D, or E), which corresponds to the answer chosen by the model.\\n\\t\"prediction_text\": string  // This is the answer to the question. It is the text corresponding to the letter chosen as the answer. It is usually one word or a short phrase.\\n\\t\"explanation\": string  // This is the explanation for the prediction. It is a piece of text explains why the model chose the answer it did.\\n}\\n\\n', additional_kwargs={}, example=False)],\n",
       " [HumanMessage(content='\\nQuestion: What do people typically do while playing guitar?\\n\\n\\nChoices:\\n-A: cry\\n-B: hear sounds\\n-C: singing\\n-D: arthritis\\n-E: making music\\n\\n\\nLet\\'s think step by step.\\n\\n\\n\\n```json\\n{\\n\\t\"prediction_letter\": string  // This is the answer to the question. It is one letter (either A, B, C, D, or E), which corresponds to the answer chosen by the model.\\n\\t\"prediction_text\": string  // This is the answer to the question. It is the text corresponding to the letter chosen as the answer. It is usually one word or a short phrase.\\n\\t\"explanation\": string  // This is the explanation for the prediction. It is a piece of text explains why the model chose the answer it did.\\n}\\n\\n', additional_kwargs={}, example=False)]]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompts = []\n",
    "\n",
    "for instance in data:\n",
    "    prompt_lc = new_prompt_template.format_messages(\n",
    "        question=instance[\"stem\"],\n",
    "        choice_A=instance[\"choice_A\"],\n",
    "        choice_B=instance[\"choice_B\"],\n",
    "        choice_C=instance[\"choice_C\"],\n",
    "        choice_D=instance[\"choice_D\"],\n",
    "        choice_E=instance[\"choice_E\"],\n",
    "        format_instructions=format_instructions\n",
    "    )\n",
    "    prompts.append(prompt_lc)\n",
    "\n",
    "prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\\n\\t\"prediction_letter\": \"A\",\\n\\t\"prediction_text\": \"bank\",\\n\\t\"explanation\": \"A revolving door is often used as a security measure at a bank to control the flow of people entering and exiting the building. It helps prevent unauthorized access and provides a controlled entry point for security purposes.\"\\n}'"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = llm(prompts[0])\n",
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Responses:\n",
      "{0: '{\\n'\n",
      "    '\\t\"prediction_letter\": \"A\",\\n'\n",
      "    '\\t\"prediction_text\": \"bank\",\\n'\n",
      "    '\\t\"explanation\": \"A revolving door is often used as a security measure at '\n",
      "    'a bank to control the flow of people entering and exiting the building. '\n",
      "    'It helps prevent unauthorized access and provides a controlled entry '\n",
      "    'point for security purposes.\"\\n'\n",
      "    '}',\n",
      " 1: '{\\n'\n",
      "    '\\t\"prediction_letter\": \"A\",\\n'\n",
      "    '\\t\"prediction_text\": \"complete job\",\\n'\n",
      "    '\\t\"explanation\": \"People aim to complete their job tasks and '\n",
      "    'responsibilities at work. This includes meeting deadlines, achieving '\n",
      "    'goals, and delivering results.\"\\n'\n",
      "    '}',\n",
      " 2: '{\\n'\n",
      "    '\\t\"prediction_letter\": \"B\",\\n'\n",
      "    '\\t\"prediction_text\": \"bookstore\",\\n'\n",
      "    '\\t\"explanation\": \"Magazines are commonly found in bookstores, where they '\n",
      "    'are displayed alongside many other printed works such as books and '\n",
      "    'newspapers.\"\\n'\n",
      "    '}',\n",
      " 3: '{\\n'\n",
      "    '\\t\"prediction_letter\": \"A\",\\n'\n",
      "    '\\t\"prediction_text\": \"fast food restaurant\",\\n'\n",
      "    '\\t\"explanation\": \"The most likely place to find a hamburger is at a fast '\n",
      "    'food restaurant, as they are known for serving hamburgers as one of their '\n",
      "    'main menu items.\"\\n'\n",
      "    '}',\n",
      " 4: '{\\n'\n",
      "    '\\t\"prediction_letter\": \"D\",\\n'\n",
      "    '\\t\"prediction_text\": \"farming areas\",\\n'\n",
      "    '\\t\"explanation\": \"James is looking for farmland, so he should look in '\n",
      "    'areas where farming is common. Therefore, he should look in farming '\n",
      "    'areas.\"\\n'\n",
      "    '}',\n",
      " 5: '{\\n'\n",
      "    '\\t\"prediction_letter\": \"C\",\\n'\n",
      "    '\\t\"prediction_text\": \"great britain\",\\n'\n",
      "    '\\t\"explanation\": \"The model predicts that the island country where ferret '\n",
      "    'is popular is Great Britain. This is because ferrets are commonly kept as '\n",
      "    'pets in Great Britain and are popular among pet owners.\"\\n'\n",
      "    '}',\n",
      " 6: '{\\n'\n",
      "    '\\t\"prediction_letter\": \"B\",\\n'\n",
      "    '\\t\"prediction_text\": \"mexico\",\\n'\n",
      "    '\\t\"explanation\": \"The correct answer is B: mexico. Mexico is a Spanish '\n",
      "    \"speaking North American country known for its excellent coffee. Mildred's \"\n",
      "    'coffee shop, diner, kitchen, and canteen are not specific to any country '\n",
      "    'and do not guarantee a great cup of coffee.\"\\n'\n",
      "    '}',\n",
      " 7: '{\\n'\n",
      "    '\\t\"prediction_letter\": \"D\",\\n'\n",
      "    '\\t\"prediction_text\": \"listen to each other\",\\n'\n",
      "    '\\t\"explanation\": \"When an enemy is approaching, animals often listen to '\n",
      "    'each other to communicate and coordinate their actions. This helps them '\n",
      "    'stay alert and respond effectively to the threat.\"\\n'\n",
      "    '}',\n",
      " 8: '{\\n'\n",
      "    '\\t\"prediction_letter\": \"A\",\\n'\n",
      "    '\\t\"prediction_text\": \"literacy\",\\n'\n",
      "    '\\t\"explanation\": \"Reading newspaper is one of the many ways to practice '\n",
      "    'literacy, which refers to the ability to read and write. By reading '\n",
      "    'newspaper, you can improve your reading skills, expand your vocabulary, '\n",
      "    'and enhance your comprehension abilities.\"\\n'\n",
      "    '}',\n",
      " 9: '{\\n'\n",
      "    '\\t\"prediction_letter\": \"E\",\\n'\n",
      "    '\\t\"prediction_text\": \"making music\",\\n'\n",
      "    '\\t\"explanation\": \"When people play the guitar, they typically make music '\n",
      "    'by strumming or picking the strings to produce different notes and '\n",
      "    'chords. This involves playing melodies, harmonies, and rhythms, which '\n",
      "    'collectively create music.\"\\n'\n",
      "    '}'}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "responses = {}\n",
    "\n",
    "for idx, prompt in enumerate(prompts):\n",
    "    response = llm(prompt)\n",
    "    responses[idx] = response.content\n",
    "\n",
    "print(f\"Responses:\\n{pp.pformat(responses)}\\n\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### change explanation schema\n",
    "\n",
    "- the output formatting instructions affect the generation - probably too much.. parse output in a different call? or using a different method? \n",
    "- maybe I can use another model to only extract the letter (which is what I need for evaluation of predictions) and treat the full generation as the explanation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n```json\\n{\\n\\t\"prediction_letter\": string  // This is the answer to the question. It is one letter (either A, B, C, D, or E), which corresponds to the answer chosen by the model.\\n\\t\"prediction_text\": string  // This is the answer to the question. It is the text corresponding to the letter chosen as the answer. It is usually one word or a short phrase.\\n\\t\"full_generation\": string  // This is the full text generated by the model.\\n}\\n'"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "explanation_schema = ResponseSchema(\n",
    "    name=\"full_generation\", \n",
    "    description=\"This is the full text generated by the model.\"\n",
    ")\n",
    "\n",
    "schemas = [prediction_letter_schema, prediction_text_schema, explanation_schema]\n",
    "\n",
    "output_parser = StructuredOutputParser.from_response_schemas(schemas)\n",
    "\n",
    "format_instructions = output_parser.get_format_instructions(only_json=True)\n",
    "format_instructions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[HumanMessage(content='\\nQuestion: A revolving door is convenient for two direction travel, but it also serves as a security measure at a what?\\n\\n\\nChoices:\\n-A: bank\\n-B: library\\n-C: department store\\n-D: mall\\n-E: new york\\n\\n\\nLet\\'s think step by step.\\n\\n\\n\\n```json\\n{\\n\\t\"prediction_letter\": string  // This is the answer to the question. It is one letter (either A, B, C, D, or E), which corresponds to the answer chosen by the model.\\n\\t\"prediction_text\": string  // This is the answer to the question. It is the text corresponding to the letter chosen as the answer. It is usually one word or a short phrase.\\n\\t\"full_generation\": string  // This is the full text generated by the model.\\n}\\n\\n', additional_kwargs={}, example=False)],\n",
       " [HumanMessage(content='\\nQuestion: What do people aim to do at work?\\n\\n\\nChoices:\\n-A: complete job\\n-B: learn from each other\\n-C: kill animals\\n-D: wear hats\\n-E: talk to each other\\n\\n\\nLet\\'s think step by step.\\n\\n\\n\\n```json\\n{\\n\\t\"prediction_letter\": string  // This is the answer to the question. It is one letter (either A, B, C, D, or E), which corresponds to the answer chosen by the model.\\n\\t\"prediction_text\": string  // This is the answer to the question. It is the text corresponding to the letter chosen as the answer. It is usually one word or a short phrase.\\n\\t\"full_generation\": string  // This is the full text generated by the model.\\n}\\n\\n', additional_kwargs={}, example=False)],\n",
       " [HumanMessage(content='\\nQuestion: Where would you find magazines along side many other printed works?\\n\\n\\nChoices:\\n-A: doctor\\n-B: bookstore\\n-C: market\\n-D: train station\\n-E: mortuary\\n\\n\\nLet\\'s think step by step.\\n\\n\\n\\n```json\\n{\\n\\t\"prediction_letter\": string  // This is the answer to the question. It is one letter (either A, B, C, D, or E), which corresponds to the answer chosen by the model.\\n\\t\"prediction_text\": string  // This is the answer to the question. It is the text corresponding to the letter chosen as the answer. It is usually one word or a short phrase.\\n\\t\"full_generation\": string  // This is the full text generated by the model.\\n}\\n\\n', additional_kwargs={}, example=False)],\n",
       " [HumanMessage(content='\\nQuestion: Where are  you likely to find a hamburger?\\n\\n\\nChoices:\\n-A: fast food restaurant\\n-B: pizza\\n-C: ground up dead cows\\n-D: mouth\\n-E: cow carcus\\n\\n\\nLet\\'s think step by step.\\n\\n\\n\\n```json\\n{\\n\\t\"prediction_letter\": string  // This is the answer to the question. It is one letter (either A, B, C, D, or E), which corresponds to the answer chosen by the model.\\n\\t\"prediction_text\": string  // This is the answer to the question. It is the text corresponding to the letter chosen as the answer. It is usually one word or a short phrase.\\n\\t\"full_generation\": string  // This is the full text generated by the model.\\n}\\n\\n', additional_kwargs={}, example=False)],\n",
       " [HumanMessage(content='\\nQuestion: James was looking for a good place to buy farmland.  Where might he look?\\n\\n\\nChoices:\\n-A: midwest\\n-B: countryside\\n-C: estate\\n-D: farming areas\\n-E: illinois\\n\\n\\nLet\\'s think step by step.\\n\\n\\n\\n```json\\n{\\n\\t\"prediction_letter\": string  // This is the answer to the question. It is one letter (either A, B, C, D, or E), which corresponds to the answer chosen by the model.\\n\\t\"prediction_text\": string  // This is the answer to the question. It is the text corresponding to the letter chosen as the answer. It is usually one word or a short phrase.\\n\\t\"full_generation\": string  // This is the full text generated by the model.\\n}\\n\\n', additional_kwargs={}, example=False)],\n",
       " [HumanMessage(content='\\nQuestion: What island country is ferret popular?\\n\\n\\nChoices:\\n-A: own home\\n-B: north carolina\\n-C: great britain\\n-D: hutch\\n-E: outdoors\\n\\n\\nLet\\'s think step by step.\\n\\n\\n\\n```json\\n{\\n\\t\"prediction_letter\": string  // This is the answer to the question. It is one letter (either A, B, C, D, or E), which corresponds to the answer chosen by the model.\\n\\t\"prediction_text\": string  // This is the answer to the question. It is the text corresponding to the letter chosen as the answer. It is usually one word or a short phrase.\\n\\t\"full_generation\": string  // This is the full text generated by the model.\\n}\\n\\n', additional_kwargs={}, example=False)],\n",
       " [HumanMessage(content='\\nQuestion: In what Spanish speaking North American country can you get a great cup of coffee?\\n\\n\\nChoices:\\n-A: mildred\\'s coffee shop\\n-B: mexico\\n-C: diner\\n-D: kitchen\\n-E: canteen\\n\\n\\nLet\\'s think step by step.\\n\\n\\n\\n```json\\n{\\n\\t\"prediction_letter\": string  // This is the answer to the question. It is one letter (either A, B, C, D, or E), which corresponds to the answer chosen by the model.\\n\\t\"prediction_text\": string  // This is the answer to the question. It is the text corresponding to the letter chosen as the answer. It is usually one word or a short phrase.\\n\\t\"full_generation\": string  // This is the full text generated by the model.\\n}\\n\\n', additional_kwargs={}, example=False)],\n",
       " [HumanMessage(content='\\nQuestion: What do animals do when an enemy is approaching?\\n\\n\\nChoices:\\n-A: feel pleasure\\n-B: procreate\\n-C: pass water\\n-D: listen to each other\\n-E: sing\\n\\n\\nLet\\'s think step by step.\\n\\n\\n\\n```json\\n{\\n\\t\"prediction_letter\": string  // This is the answer to the question. It is one letter (either A, B, C, D, or E), which corresponds to the answer chosen by the model.\\n\\t\"prediction_text\": string  // This is the answer to the question. It is the text corresponding to the letter chosen as the answer. It is usually one word or a short phrase.\\n\\t\"full_generation\": string  // This is the full text generated by the model.\\n}\\n\\n', additional_kwargs={}, example=False)],\n",
       " [HumanMessage(content='\\nQuestion: Reading newspaper one of many ways to practice your what?\\n\\n\\nChoices:\\n-A: literacy\\n-B: knowing how to read\\n-C: money\\n-D: buying\\n-E: money bank\\n\\n\\nLet\\'s think step by step.\\n\\n\\n\\n```json\\n{\\n\\t\"prediction_letter\": string  // This is the answer to the question. It is one letter (either A, B, C, D, or E), which corresponds to the answer chosen by the model.\\n\\t\"prediction_text\": string  // This is the answer to the question. It is the text corresponding to the letter chosen as the answer. It is usually one word or a short phrase.\\n\\t\"full_generation\": string  // This is the full text generated by the model.\\n}\\n\\n', additional_kwargs={}, example=False)],\n",
       " [HumanMessage(content='\\nQuestion: What do people typically do while playing guitar?\\n\\n\\nChoices:\\n-A: cry\\n-B: hear sounds\\n-C: singing\\n-D: arthritis\\n-E: making music\\n\\n\\nLet\\'s think step by step.\\n\\n\\n\\n```json\\n{\\n\\t\"prediction_letter\": string  // This is the answer to the question. It is one letter (either A, B, C, D, or E), which corresponds to the answer chosen by the model.\\n\\t\"prediction_text\": string  // This is the answer to the question. It is the text corresponding to the letter chosen as the answer. It is usually one word or a short phrase.\\n\\t\"full_generation\": string  // This is the full text generated by the model.\\n}\\n\\n', additional_kwargs={}, example=False)]]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompts = []\n",
    "\n",
    "for instance in data:\n",
    "    prompt_lc = new_prompt_template.format_messages(\n",
    "        question=instance[\"stem\"],\n",
    "        choice_A=instance[\"choice_A\"],\n",
    "        choice_B=instance[\"choice_B\"],\n",
    "        choice_C=instance[\"choice_C\"],\n",
    "        choice_D=instance[\"choice_D\"],\n",
    "        choice_E=instance[\"choice_E\"],\n",
    "        format_instructions=format_instructions\n",
    "    )\n",
    "    prompts.append(prompt_lc)\n",
    "\n",
    "prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: {'prediction_letter': 'A',\n",
       "  'prediction_text': 'bank',\n",
       "  'full_generation': 'A revolving door is convenient for two direction travel, but it also serves as a security measure at a bank.'},\n",
       " 1: {'prediction_letter': 'A',\n",
       "  'prediction_text': 'complete job',\n",
       "  'full_generation': 'People aim to complete their job at work. This involves fulfilling their responsibilities, meeting deadlines, and achieving the tasks assigned to them.'},\n",
       " 2: {'prediction_letter': 'B',\n",
       "  'prediction_text': 'bookstore',\n",
       "  'full_generation': 'The answer is B: bookstore. Magazines can be found in bookstores along with many other printed works.'},\n",
       " 3: {'prediction_letter': 'A',\n",
       "  'prediction_text': 'fast food restaurant',\n",
       "  'full_generation': 'The most likely place to find a hamburger is at a fast food restaurant.'},\n",
       " 4: {'prediction_letter': 'D',\n",
       "  'prediction_text': 'farming areas',\n",
       "  'full_generation': 'James should look for farmland in farming areas. These areas are specifically designated for agriculture and would be the best place to find suitable farmland for purchase.'}}"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "responses = {}\n",
    "\n",
    "for idx, prompt in enumerate(prompts):\n",
    "    if idx < 5:\n",
    "        response = llm(prompt)\n",
    "        responses[idx] = output_parser.parse(response.content)\n",
    "responses"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis-venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
