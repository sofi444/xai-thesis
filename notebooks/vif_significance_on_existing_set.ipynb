{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Statistical Tests (VIF and P-value) for Feature Sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VIF\n",
    "\n",
    "Variance Inflation Factor (VIF) is a measure used to assess multicollinearity in regression analysis. It quantifies how much the variance of the estimated regression coefficient is increased due to the correlation between predictor variables. A high VIF value indicates a strong correlation between predictor variables, which can lead to unreliable and unstable regression coefficient estimates. In other words, VIF helps identify predictors that are highly correlated with each other, which can cause issues in the interpretation and reliability of the regression model.\n",
    "\n",
    "- 1 = not correlated.\n",
    "- Between 1 and 5 = moderately correlated.\n",
    "- Greater than 5 = highly correlated.\n",
    "\n",
    "### P-value\n",
    "\n",
    "P-value is used to determine the statistical significance of each predictor (or independent variable) in the model. The p-value is a measure of the probability that an observed difference could have occurred just by random chance. If the p-value is less than a certain significance level (commonly 0.05), then we conclude that the predictor is statistically significant.\n",
    "\n",
    "- Small is better as it indicates that a result did not take place by chance\n",
    "- Smaller means the null hypothesis (that there is no relationship exists between two variables)\n",
    "- smaller than 0.05 indicates significance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import json\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import sys\n",
    "from datasets import load_from_disk\n",
    "\n",
    "sys.path.append(\"../\")\n",
    "import utils.features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/mount/studenten-temp1/users/dpgo/xai-thesis'"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PROJECT_DIR = os.path.dirname(os.getcwd())\n",
    "STATS_DIR = os.path.join(PROJECT_DIR, 'classification/stats')\n",
    "FEATURES_DIR = os.path.join(PROJECT_DIR, 'feature_extraction/features/')\n",
    "RESPONSES_DIR = os.path.join(PROJECT_DIR, 'responses/')\n",
    "SPLITS_DIR = os.path.join(PROJECT_DIR, \"classification/split_datasets/coqa\")\n",
    "PROJECT_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "existing_splits = True\n",
    "\n",
    "def load_x_y(features_filename, responses_filename):\n",
    "    \n",
    "    # load features (x)\n",
    "    features_df = utils.features.load_features(features_filename)\n",
    "\n",
    "    # load labels (y)\n",
    "    labels_df = utils.features.load_labels(responses_filename)\n",
    "\n",
    "    # merge\n",
    "    data_df = utils.features.merge_and_filter(features_df, labels_df)\n",
    "\n",
    "    # for testing: 15 rows, 15 columns\n",
    "    #data_df = data_df.iloc[-15:, -15:] # tmp\n",
    "\n",
    "    # normalize + select features\n",
    "    data_df = manipulate_features(data_df)\n",
    "\n",
    "    print(f\"Instances: {len(data_df)}\\nFeatures: {len(data_df.columns)-1}\")\n",
    "\n",
    "    return data_df\n",
    "\n",
    "\n",
    "def manipulate_features(data_df):\n",
    "    # normalize features\n",
    "    data_df = utils.features.scale_features(\n",
    "        data_df, \n",
    "        scaler=MinMaxScaler(\n",
    "            feature_range=(0, 1)\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    return data_df\n",
    "\n",
    "\n",
    "def prepare_splits(data_df, test_size=0.2, random_state=1):\n",
    "\n",
    "    if existing_splits: \n",
    "        print(\"\\nUsing existing splits\")\n",
    "        # use exact same split as in the SPLITS_DIR\n",
    "        raw_dataset = load_from_disk(SPLITS_DIR)\n",
    "        # eg. train_df contains instances in raw_dataset['train']\n",
    "        # based on the pandas_idx in raw_dataset['train']['pandas_idx']\n",
    "        train_df = data_df[data_df.index.isin(raw_dataset['train']['pandas_idx'])]\n",
    "        test_df = data_df[data_df.index.isin(raw_dataset['test']['pandas_idx'])]\n",
    "    else:\n",
    "        print(\"\\nCreating new splits\")\n",
    "        train_df, test_df = train_test_split(\n",
    "            data_df, \n",
    "            test_size=test_size, \n",
    "            random_state=random_state\n",
    "        )\n",
    "\n",
    "    X_train = train_df.drop(columns=['outcome'])\n",
    "    y_train = train_df['outcome']\n",
    "\n",
    "    X_test = test_df.drop(columns=['outcome'])\n",
    "    y_test = test_df['outcome']\n",
    "\n",
    "    print('X_train shape:', X_train.shape)\n",
    "    print('y_train shape:', y_train.shape)\n",
    "    print('X_test shape:', X_test.shape)\n",
    "    print('y_test shape:', y_test.shape)\n",
    "\n",
    "    return X_train, y_train, X_test, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## col-RFE feature set\n",
    "\n",
    "- 300 features\n",
    "\n",
    "1. remove collinear features\n",
    "2. Recursive Feature Elimination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_set_source = os.path.join(STATS_DIR, \"12091031_col-rfe_all_10000.json\")\n",
    "with open(feature_set_source) as f:\n",
    "    stats = json.load(f)\n",
    "    feature_set = [f for f in stats['coefficients'].keys()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instances: 10000\n",
      "Features: 2371\n",
      "\n",
      "Using existing splits\n",
      "X_train shape: (8000, 2371)\n",
      "y_train shape: (8000,)\n",
      "X_test shape: (1000, 2371)\n",
      "y_test shape: (1000,)\n"
     ]
    }
   ],
   "source": [
    "data_df = load_x_y(\n",
    "    features_filename=os.path.join(FEATURES_DIR, '12091031_all_features.csv.gz'),\n",
    "    responses_filename=os.path.join(RESPONSES_DIR, '12091031_parsed_turbo_10000_eval.jsonl')\n",
    ")\n",
    "X_train, y_train, X_test, y_test = prepare_splits(data_df, test_size=0.2, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (8000, 300) X_test shape: (1000, 300)\n"
     ]
    }
   ],
   "source": [
    "# Only use features in feature set\n",
    "X_train = X_train[feature_set]\n",
    "X_test = X_test[feature_set]\n",
    "\n",
    "print('X_train shape:', X_train.shape, 'X_test shape:', X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((8000, 299), (1000, 299))"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "constant_features = X_train.columns[X_train.nunique() == 1] # constant features \n",
    "\n",
    "X_train = X_train.drop(columns=constant_features)\n",
    "X_test = X_test.drop(columns=constant_features)\n",
    "\n",
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check for multicollinearity - there should be none (already removed highly correlated features)\n",
    "corr_matrix = X_train.corr().abs()\n",
    "upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n",
    "to_drop = [column for column in upper.columns if any(upper[column] > 0.80)]\n",
    "to_drop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "coefficients = stats['coefficients']\n",
    "\n",
    "threshold = 0.1\n",
    "large_coefficients = {k: v for k, v in coefficients.items() if abs(v) > threshold}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(300, 297)"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(coefficients), len(large_coefficients)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "\n",
    "# calculate VIF and p-value\n",
    "\n",
    "# a feature is robust if it has a low p-value and low VIF (and a large coefficient)\n",
    "robust_features = {} # individual features and interactions\n",
    "\n",
    "for feature, coef in large_coefficients.items():\n",
    "\n",
    "    X = sm.add_constant(X_train[[feature]]) # stats models allows us to get the p-values\n",
    "    X.reset_index(drop=True, inplace=True) # avoid index mismatch error\n",
    "    y = y_train.reset_index(drop=True) # avoid index mismatch error\n",
    "    \n",
    "    sm_model = sm.Logit(y, X)\n",
    "    result = sm_model.fit(method='lbfgs', maxiter=1000, disp=False)\n",
    "\n",
    "    p_value = result.pvalues[1]\n",
    "    \n",
    "    vif = variance_inflation_factor(X.values, 1) # idx of col to check (0: constant, 1: feature since we add them one by one)\n",
    "\n",
    "    if p_value < 0.05 and vif < 5: # robust and significant\n",
    "        robust_features[feature] = {\n",
    "            'coefficient': round(coef, 5), \n",
    "            'p_value': round(p_value, 5), \n",
    "            'vif': round(vif, 5)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "108"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(robust_features.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-8 {color: black;}#sk-container-id-8 pre{padding: 0;}#sk-container-id-8 div.sk-toggleable {background-color: white;}#sk-container-id-8 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-8 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-8 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-8 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-8 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-8 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-8 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-8 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-8 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-8 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-8 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-8 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-8 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-8 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-8 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-8 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-8 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-8 div.sk-item {position: relative;z-index: 1;}#sk-container-id-8 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-8 div.sk-item::before, #sk-container-id-8 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-8 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-8 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-8 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-8 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-8 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-8 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-8 div.sk-label-container {text-align: center;}#sk-container-id-8 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-8 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-8\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression(max_iter=1000, random_state=1)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-8\" type=\"checkbox\" checked><label for=\"sk-estimator-id-8\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression(max_iter=1000, random_state=1)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LogisticRegression(max_iter=1000, random_state=1)"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "model = LogisticRegression(\n",
    "    penalty='l2', \n",
    "    C=1.0, \n",
    "    solver='lbfgs', \n",
    "    max_iter=1000, \n",
    "    random_state=1\n",
    ")\n",
    "\n",
    "model.fit(X_train[list(robust_features.keys())], y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test[list(robust_features.keys())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 1000)"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(y_pred), len(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.61      0.44      0.51       378\n",
      "        True       0.71      0.83      0.76       622\n",
      "\n",
      "    accuracy                           0.68      1000\n",
      "   macro avg       0.66      0.63      0.64      1000\n",
      "weighted avg       0.67      0.68      0.67      1000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "robust_features_names = [f for f in robust_features.keys()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Notlw_Lasswell_nouns': -2.92866,\n",
       " 'Know_GI': -2.52436,\n",
       " 'Quan_GI_neg_3': -2.25218,\n",
       " 'attention_neg_3': -1.58035,\n",
       " 'Rsploss_Lasswell_adjectives': -1.52251,\n",
       " 'Causal_GI_verbs': -1.3337,\n",
       " 'If_Lasswell': -1.20076,\n",
       " 'quality': -1.17408,\n",
       " 'basic_nfunction_types': -1.16112,\n",
       " 'Aquatic_GI_adjectives': -1.14377,\n",
       " 'Passive_GI_verbs': -1.1242,\n",
       " 'Emot_GI_verbs_neg_3': -1.12328,\n",
       " 'hu_liu_prop_verbs': -1.1113,\n",
       " 'Powoth_Lasswell': -1.08598,\n",
       " 'Anxiety_GALC_adjectives': -1.03909,\n",
       " 'Skloth_Lasswell_adjectives': -1.01587,\n",
       " 'Submit_GI': -1.0014,\n",
       " 'Say_GI_neg_3': 1.01424,\n",
       " 'Ord_GI_adjectives_neg_3': 1.03378,\n",
       " 'ADP': 1.0422,\n",
       " 'Rcethic_Lasswell_verbs': 1.0821,\n",
       " 'interactivity_2': 1.16675,\n",
       " 'Tool_GI': 1.20594,\n",
       " 'Persist_GI_nouns_neg_3': 1.24415,\n",
       " 'NOUN': 1.25014,\n",
       " 'Yes_GI': 1.28595,\n",
       " 'argumentative': 1.38458,\n",
       " 'Eval_GI_adverbs_neg_3': 1.41003,\n",
       " 'VERB': 1.43709,\n",
       " 'Powcoop_Lasswell_adverbs': 1.48853,\n",
       " 'ttr': 1.53951,\n",
       " 'Know_GI_verbs': 1.94893}"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# make it easier to read the list of robust features (only show the ones with a very large coef -> likely to be important)\n",
    "threshold = 1\n",
    "\n",
    "pretty_robust_features = {k: v for k, v in robust_features.items() if abs(v['coefficient']) > threshold}\n",
    "pretty_robust_features = {k: v['coefficient'] for k, v in sorted(pretty_robust_features.items(), key=lambda item: item[1]['coefficient'])}\n",
    "\n",
    "pretty_robust_features_names = [f for f in pretty_robust_features.keys()]\n",
    "pretty_robust_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save robust features dict (has features name, pvalue and vif for each)\n",
    "with open(os.path.join(STATS_DIR, \"robust_features_col-rfe.json\"), 'w') as f:\n",
    "    json.dump(robust_features, f, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RFE feature set\n",
    "\n",
    "- more features (711)\n",
    "\n",
    "1. Recursive Feature Elimination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instances: 10000\n",
      "Features: 2371\n",
      "\n",
      "Using existing splits\n",
      "X_train shape: (8000, 2371)\n",
      "y_train shape: (8000,)\n",
      "X_test shape: (1000, 2371)\n",
      "y_test shape: (1000,)\n"
     ]
    }
   ],
   "source": [
    "data_df = load_x_y(\n",
    "    features_filename=os.path.join(FEATURES_DIR, '12091031_all_features.csv.gz'),\n",
    "    responses_filename=os.path.join(RESPONSES_DIR, '12091031_parsed_turbo_10000_eval.jsonl')\n",
    ")\n",
    "X_train, y_train, X_test, y_test = prepare_splits(data_df, test_size=0.2, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_set_source = os.path.join(STATS_DIR, \"12091031_rfe_all_10000.json\")\n",
    "with open(feature_set_source) as f:\n",
    "    stats = json.load(f)\n",
    "    feature_set = [f for f in stats['coefficients'].keys()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (8000, 711) X_test shape: (1000, 711)\n"
     ]
    }
   ],
   "source": [
    "# Only use features in feature set\n",
    "X_train = X_train[feature_set]\n",
    "X_test = X_test[feature_set]\n",
    "\n",
    "print('X_train shape:', X_train.shape, 'X_test shape:', X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((8000, 709), (1000, 709))"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "constant_features = X_train.columns[X_train.nunique() == 1] # constant features \n",
    "\n",
    "X_train = X_train.drop(columns=constant_features)\n",
    "X_test = X_test.drop(columns=constant_features)\n",
    "\n",
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "177"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check for multicollinearity - in RFE only setting, we have not alredy removed collinear features\n",
    "corr_matrix = X_train.corr().abs()\n",
    "upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n",
    "to_drop = [column for column in upper.columns if any(upper[column] > 0.90)]\n",
    "\n",
    "# keep copy of X_train and X_test with collinear features\n",
    "X_train_with_corr = X_train.copy()\n",
    "X_test_with_corr = X_test.copy()\n",
    "\n",
    "# drop features in to_drop\n",
    "X_train = X_train.drop(columns=to_drop)\n",
    "X_test = X_test.drop(columns=to_drop)\n",
    "\n",
    "len(to_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "coefficients = stats['coefficients']\n",
    "\n",
    "threshold = 0.3\n",
    "large_coefficients = {k: v for k, v in coefficients.items() if abs(v) > threshold}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(711, 617)"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(coefficients), len(large_coefficients)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "\n",
    "# calculate VIF and p-value\n",
    "\n",
    "# a feature is robust if it has a low p-value and low VIF (and a large coefficient)\n",
    "robust_features = {} # individual features and interactions\n",
    "\n",
    "for feature, coef in large_coefficients.items():\n",
    "\n",
    "    if feature in to_drop: # we've dropped these\n",
    "        continue\n",
    "\n",
    "    X = sm.add_constant(X_train[[feature]]) # stats models allows us to get the p-values\n",
    "    X.reset_index(drop=True, inplace=True) # avoid index mismatch error\n",
    "    y = y_train.reset_index(drop=True) # avoid index mismatch error\n",
    "    \n",
    "    sm_model = sm.Logit(y, X)\n",
    "    result = sm_model.fit(method='lbfgs', maxiter=1000, disp=False)\n",
    "\n",
    "    p_value = result.pvalues[1]\n",
    "    \n",
    "    vif = variance_inflation_factor(X.values, 1) # idx of col to check (0: constant, 1: feature since we add them one by one)\n",
    "\n",
    "    if p_value < 0.05 and vif < 5: # robust and significant\n",
    "        robust_features[feature] = {\n",
    "            'coefficient': round(coef, 5), \n",
    "            'p_value': round(p_value, 5), \n",
    "            'vif': round(vif, 5)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "176"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(robust_features.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-9 {color: black;}#sk-container-id-9 pre{padding: 0;}#sk-container-id-9 div.sk-toggleable {background-color: white;}#sk-container-id-9 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-9 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-9 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-9 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-9 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-9 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-9 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-9 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-9 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-9 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-9 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-9 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-9 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-9 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-9 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-9 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-9 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-9 div.sk-item {position: relative;z-index: 1;}#sk-container-id-9 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-9 div.sk-item::before, #sk-container-id-9 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-9 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-9 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-9 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-9 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-9 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-9 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-9 div.sk-label-container {text-align: center;}#sk-container-id-9 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-9 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-9\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression(max_iter=1000, random_state=1)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-9\" type=\"checkbox\" checked><label for=\"sk-estimator-id-9\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression(max_iter=1000, random_state=1)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LogisticRegression(max_iter=1000, random_state=1)"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "model = LogisticRegression(\n",
    "    penalty='l2', \n",
    "    C=1.0, \n",
    "    solver='lbfgs', \n",
    "    max_iter=1000, \n",
    "    random_state=1\n",
    ")\n",
    "\n",
    "model.fit(X_train[list(robust_features.keys())], y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test[list(robust_features.keys())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 1000)"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(y_pred), len(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.63      0.47      0.53       378\n",
      "        True       0.72      0.83      0.77       622\n",
      "\n",
      "    accuracy                           0.69      1000\n",
      "   macro avg       0.67      0.65      0.65      1000\n",
      "weighted avg       0.69      0.69      0.68      1000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "robust_features_names = [f for f in robust_features.keys()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Negate_GI_nouns': -2.05529,\n",
       " 'Notlw_Lasswell_nouns': -1.83287,\n",
       " 'Arousal_nwords_adjectives': -1.27769,\n",
       " 'Socrel_GI_verbs_neg_3': -1.2499,\n",
       " 'Rel_GI_adjectives_neg_3': -1.22962,\n",
       " 'Increas_GI_adjectives': -1.20249,\n",
       " 'Role_GI_adjectives_neg_3': -1.19062,\n",
       " 'Know_GI': -1.11482,\n",
       " 'Econ_GI': -1.09186,\n",
       " 'Quan_GI_adjectives': -1.05305,\n",
       " 'Weak_GI_nouns_neg_3': -1.04468,\n",
       " 'Ngtv_GI_adjectives': -1.03029,\n",
       " 'Enltot_Lasswell_verbs': -1.02434,\n",
       " 'attention': -1.01314,\n",
       " 'Pleasur_GI_verbs': -1.01109,\n",
       " 'Timespc_Lasswell_verbs': -1.00439,\n",
       " 'hu_liu_neg_perc_nouns': -1.00075,\n",
       " 'interactivity_0': -0.9578,\n",
       " 'Skltot_Lasswell_adjectives_neg_3': -0.95241,\n",
       " 'Ptlw_Lasswell_verbs': -0.94743,\n",
       " 'No_GI_neg_3': -0.94025,\n",
       " 'Endslw_Lasswell_adverbs_neg_3': -0.90483,\n",
       " 'Powoth_Lasswell': -0.89708,\n",
       " 'hu_liu_prop_verbs_neg_3': -0.89697,\n",
       " 'Rsploss_Lasswell_adjectives': -0.89427,\n",
       " 'If_Lasswell': -0.86375,\n",
       " 'Surelw_Lasswell_verbs': -0.86285,\n",
       " 'Com_GI_neg_3': -0.83167,\n",
       " 'aptitude_nouns_neg_3': -0.82251,\n",
       " 'Quan_GI': -0.80654,\n",
       " 'Anticipation_EmoLex_adjectives_neg_3': -0.80123,\n",
       " 'interactivity_3': -0.80027,\n",
       " 'Powpt_Lasswell_verbs': 0.82334,\n",
       " 'Time_GI_neg_3': 0.85621,\n",
       " 'attention_adjectives': 0.88221,\n",
       " 'Com_GI_nouns': 0.88945,\n",
       " 'lexical_density_types': 0.89022,\n",
       " 'Amusement_GALC_verbs_neg_3': 0.89355,\n",
       " 'Wlttran_Lasswell_neg_3': 0.90749,\n",
       " 'Ought_GI_verbs': 0.93061,\n",
       " 'VERB': 0.95028,\n",
       " 'Tool_GI': 0.96073,\n",
       " 'Undrst_GI': 0.96733,\n",
       " 'Yes_GI': 0.97854,\n",
       " 'interactivity_2': 1.02057,\n",
       " 'Powpt_Lasswell_adjectives': 1.05971,\n",
       " 'ADP': 1.13596,\n",
       " 'NOUN': 1.23078,\n",
       " 'Know_GI_verbs': 1.27115,\n",
       " 'argumentative': 1.35341,\n",
       " 'ttr': 1.80164}"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# make it easier to read the list of robust features (only show the ones with a very large coef -> likely to be important)\n",
    "threshold = 0.8\n",
    "\n",
    "pretty_robust_features = {k: v for k, v in robust_features.items() if abs(v['coefficient']) > threshold}\n",
    "pretty_robust_features = {k: v['coefficient'] for k, v in sorted(pretty_robust_features.items(), key=lambda item: item[1]['coefficient'])}\n",
    "\n",
    "pretty_robust_features_names = [f for f in pretty_robust_features.keys()]\n",
    "pretty_robust_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Looking at features that are considered important across different sets\n",
    "\n",
    "see `lr_all-features_results.ipynb`\n",
    "\n",
    "- 'Notlw_Lasswell_nouns' appers in 5/5 feature sets and is robust\n",
    "\n",
    "- 'Negate_GI_nouns' appears in 4/5 feature sets (all the ones without collinear features removal as the first step; here in RFE set) and is robust - has a moderate correlation with other features, but still acceptable\n",
    "\n",
    "- 'Know_GI_neg_3' appears in 4/5 feature sets (all the ones without collinear features removal as the first step; here in RFE set) and is NOT robust. It is highly correlated with 'Know_GI'\n",
    "\n",
    "see `features/all-features_high_correlations.json` for correlations across the full feature set (all extracted features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-1.6940323288559407, True)"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coefficients['Know_GI_neg_3'], 'Know_GI_neg_3' in to_drop\n",
    "\n",
    "# This feature is considered important by the model when used, but it is collinear with other features \n",
    "# (therefore not in col-rfe feature set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Know_GI    0.965055\n",
       "Name: Know_GI_neg_3, dtype: float64"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corr_matrix = X_train_with_corr.corr().abs()\n",
    "upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n",
    "upper['Know_GI_neg_3'][upper['Know_GI_neg_3'] > 0.70] # check correlation with other features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save robust features dict (has features name, pvalue and vif for each)\n",
    "with open(os.path.join(STATS_DIR, \"robust_features_rfe.json\"), 'w') as f:\n",
    "    json.dump(robust_features, f, indent=4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis-venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
