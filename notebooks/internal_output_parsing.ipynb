{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Output Parsing\n",
    "\n",
    "+ Langchain internal (modifies the prompt with formatting instructions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv, find_dotenv\n",
    "_ = load_dotenv(find_dotenv())\n",
    "_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# an example prompt\n",
    "\n",
    "prompt_str = \"Answer the following question. There are 5 possible answer and only 1 is correct. \\\n",
    "    Question: James wanted to find an old underground map from the 50s.  Where might he look for one? \\\n",
    "    - A: library \\\n",
    "    - B: subway station \\\n",
    "    - C: county engineer's office \\\n",
    "    - D: super market \\\n",
    "    - E: home \\\n",
    "    Choose the correct answer. Let's think step by step.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<OpenAIObject chat.completion id=chatcmpl-7ddrVPFZkZEwUIm6Dpp6CkxZ2GEqA at 0x12394b4c0> JSON: {\n",
       "  \"id\": \"chatcmpl-7ddrVPFZkZEwUIm6Dpp6CkxZ2GEqA\",\n",
       "  \"object\": \"chat.completion\",\n",
       "  \"created\": 1689682377,\n",
       "  \"model\": \"gpt-3.5-turbo-0613\",\n",
       "  \"choices\": [\n",
       "    {\n",
       "      \"index\": 0,\n",
       "      \"message\": {\n",
       "        \"role\": \"assistant\",\n",
       "        \"content\": \"First, it is unlikely that James would find an old underground map from the 50s at a library or supermarket, as these places typically do not store historical documents like maps. \\n\\nNext, the county engineer's office may have records or maps related to infrastructure and planning, but it is less likely to have an underground map from the 50s specifically.\\n\\nTherefore, the best option would be to look for an old underground map from the 50s at a subway station. Subway stations often have historical displays or archives that may include maps from past decades. \\n\\nSo, the correct answer would be B: subway station.\"\n",
       "      },\n",
       "      \"finish_reason\": \"stop\"\n",
       "    }\n",
       "  ],\n",
       "  \"usage\": {\n",
       "    \"prompt_tokens\": 93,\n",
       "    \"completion_tokens\": 125,\n",
       "    \"total_tokens\": 218\n",
       "  }\n",
       "}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# normal call to the model\n",
    "\n",
    "import openai\n",
    "\n",
    "openai.ChatCompletion.create(\n",
    "  model=\"gpt-3.5-turbo\",\n",
    "  messages=[\n",
    "        {\"role\": \"system\", \n",
    "         \"content\": prompt_str\n",
    "        }\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate the model via langchain integration\n",
    "\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "chat_llm = ChatOpenAI(temperature=0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.output_parsers import StructuredOutputParser, ResponseSchema\n",
    "\n",
    "prediction_letter_schema = ResponseSchema(\n",
    "    name=\"prediction_letter\", \n",
    "    description=\"This is the answer to the question. It is one letter (either A, B, C, D, or E), which corresponds to the answer chosen by the model.\"\n",
    ")\n",
    "\n",
    "prediction_text_schema = ResponseSchema(\n",
    "    name=\"prediction_text\", \n",
    "    description=\"This is the answer to the question. It is the text of the answer chosen by the model.\"\n",
    ")\n",
    "\n",
    "explanation_schema = ResponseSchema(\n",
    "    name=\"explanation\", \n",
    "    description=\"This is the explanation for the prediction. It is a piece of text explains why the model chose the answer it did.\"\n",
    ")\n",
    "\n",
    "schemas = [prediction_letter_schema, prediction_text_schema, explanation_schema]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_parser = StructuredOutputParser.from_response_schemas(schemas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('The output should be a markdown code snippet formatted in the following '\n",
      " 'schema, including the leading and trailing \"```json\" and \"```\":\\n'\n",
      " '\\n'\n",
      " '```json\\n'\n",
      " '{\\n'\n",
      " '\\t\"prediction_letter\": string  // This is the answer to the question. It is '\n",
      " 'one letter (either A, B, C, D, or E), which corresponds to the answer chosen '\n",
      " 'by the model.\\n'\n",
      " '\\t\"prediction_text\": string  // This is the answer to the question. It is '\n",
      " 'the text of the answer chosen by the model.\\n'\n",
      " '\\t\"explanation\": string  // This is the explanation for the prediction. It '\n",
      " 'is a piece of text explains why the model chose the answer it did.\\n'\n",
      " '}\\n'\n",
      " '```')\n"
     ]
    }
   ],
   "source": [
    "format_instructions = output_parser.get_format_instructions()\n",
    "\n",
    "import pprint as pp\n",
    "\n",
    "pp.pprint(format_instructions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_question_and_choices = \"James wanted to find an old underground map from the 50s.  Where might he look for one? \\\n",
    "    - A: library \\\n",
    "    - B: subway station \\\n",
    "    - C: county engineer's office \\\n",
    "    - D: super market \\\n",
    "    - E: home\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "template = \"\"\"Answer the following question. There are 5 possible answer and only 1 is correct.\n",
    "    Question: {question_and_choices}\n",
    "    Choose the correct answer. Let's think step by step.\n",
    "    {format_instructions}\"\"\"\n",
    "\n",
    "template_lc = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "prompt_lc = template_lc.format_messages(\n",
    "    question_and_choices=test_question_and_choices,\n",
    "    format_instructions=format_instructions\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer the following question. There are 5 possible answer and only 1 is correct.\n",
      "    Question: James wanted to find an old underground map from the 50s.  Where might he look for one?     - A: library     - B: subway station     - C: county engineer's office     - D: super market     - E: home\n",
      "    Choose the correct answer. Let's think step by step.\n",
      "    The output should be a markdown code snippet formatted in the following schema, including the leading and trailing \"```json\" and \"```\":\n",
      "\n",
      "```json\n",
      "{\n",
      "\t\"prediction_letter\": string  // This is the answer to the question. It is one letter (either A, B, C, D, or E), which corresponds to the answer chosen by the model.\n",
      "\t\"prediction_text\": string  // This is the answer to the question. It is the text of the answer chosen by the model.\n",
      "\t\"explanation\": string  // This is the explanation for the prediction. It is a piece of text explains why the model chose the answer it did.\n",
      "}\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "print(prompt_lc[0].content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AIMessage(content='```json\\n{\\n\\t\"prediction_letter\": \"A\",\\n\\t\"prediction_text\": \"library\",\\n\\t\"explanation\": \"James is looking for an old underground map from the 50s. The most likely place to find such a map would be in a library, where historical documents and maps are often stored.\"\\n}\\n```', additional_kwargs={}, example=False)\n"
     ]
    }
   ],
   "source": [
    "response = chat_llm(prompt_lc)\n",
    "\n",
    "pp.pprint(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'explanation': 'James is looking for an old underground map from the 50s. The '\n",
      "                'most likely place to find such a map would be in a library, '\n",
      "                'where historical documents and maps are often stored.',\n",
      " 'prediction_letter': 'A',\n",
      " 'prediction_text': 'library'}\n"
     ]
    }
   ],
   "source": [
    "# parse the output -> structured output\n",
    "\n",
    "response_as_dict = output_parser.parse(response.content)\n",
    "\n",
    "pp.pprint(response_as_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis-venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
