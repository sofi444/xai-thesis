
# Beyond Sounding Good: Investigating Misleading Natural Language Explanations Generated by Large Language Models via Prompting

## Master's Thesis


---

**TLDR:** We frame the problem of explaining generative LLM as a binary classification task on instances of (explanation, answer) in a QA task. The classifiers learn to classify explanations that lead to a correct/incorrect answer in intrinsically different ways. Then, we interpret them by developing aggregation strategies on the output of established XAI tools.

---

While methods for XAI for generation are complex and not very interpretable for non-experts (and sometimes not viable due to the models being closed-source and/or their size), we can phrase the problem of finding patterns in the data as a classification problem.

We do this by implementing a parallel and diverse approach, where on one side we have an inherently interpretable Logistic Regression (LR) model, and on the other side, a (set of) BERT-based transformer models. We extract features using a variety of NLP and tools originally intended to assess argument quality (we assume that explanations share properties of arguments). The featurized version of the data is then fed to LR. On the other hand, BERT takes text input directly and treats tokens as features.

To understand a prediction by LR, we can look at the coefficients of the model after fitting, and to generate a global explanation, we can aggregate them across the entire test set (1000 instances). To interpret transformers, we use SHAP and develop an approach to aggregate explanations and obtain a global explanation. This is a complex problem when the input data is text. We end up with interpretations at different levels (word, ngram, chunk) of granularity of co-location of features (words).

Finally, we create plots and run in-depth analysis across different models, feature sets, and SHAP model definitions. The aim is to understand which features are important for the model to make a negative prediction (to predict that an explanation is misleading). All findings and insights can be found in the notebooks in the `analysis` directory and are explained in the thesis write up.

Practical applications of this could involves analysing explanations from LLMs to check if they contain the features that were found to be indicative of a negative prediction. If so, a warning can be given to the user that the answer given by the model might be misleading.

This approach has many possible future directions (discussed in detail in the thesis), most interestingly 1. using an open ended QA dataset (not multiple choice) for the generation step and 2. defining a different evaluation stretegy (as this affects what we define as misleading).

---

## Scripts

+ `eval_predictions.py` -- run to evaluate the LLM predictions on the QA task (after parsing)


+ `extract_features.py` -- run to extract `trad` features (combines all the tools and generated a feature file)


+ `main_generation.py` -- run to generate with the LLM on the QA task; cotrollable by passing different args; writes a (free-text) response file


+ `parse_freetext.py` -- run to parse LLM responses with CodeLlama


## Directories

+ `utils` -- contains all utils (functions) called by other scripts
+ `responses` -- all responses generated by the LLM (including parsed and evaluated versions)
+ `prompt_templates` -- prompt templates for the QA task and for output parsing with CodeLlama
+ `schemas` -- schemas for output parsing
+ `feature_extraction` -- all features and extraction tools
+ `classification` -- contains all things related to the classication task (main part)
+ `notebooks` -- mostly notebooks for playing around with idea, tools, etc., before they are integrated (also contains notebooks for statistical tests on feature sets)
+ `maps` -- just a script + json file to map instances to their unique ids and possible options
+ `data`



* `requirements.txt` is a `pip freeze >` dump of the main environment used for this project. A different version of transformers was used for finetuning (because I had to force the use of a single GPU). To run the adapters (feature extraction) a different environment was used, following the requirements in the original repository