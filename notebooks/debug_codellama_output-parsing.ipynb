{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!export \"CUDA_VISIBLE_DEVICES\"=[1,2] jupyter notebook\n",
    "\n",
    "import os\n",
    "import torch\n",
    "\n",
    "PROJECT_DIR = os.path.abspath(os.path.join(os.getcwd(), os.pardir))\n",
    "CACHE_DIR = os.path.join(PROJECT_DIR, '.cache')\n",
    "RESPONSES_DIR = os.path.join(PROJECT_DIR, 'responses')\n",
    "\n",
    "schemas_path = os.path.join(PROJECT_DIR, 'schemas/output-parsing_schemas.json')\n",
    "template_path = os.path.join(PROJECT_DIR, 'prompt_templates/output-parsing_templates.json')\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import codellama functions\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"../utils/\")\n",
    "\n",
    "import codellama\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper functions in freetext_to_format.py\n",
    "import json\n",
    "\n",
    "# set N\n",
    "def load_freetext_responses(filename:str, full_run:bool=False):\n",
    "        ''' \n",
    "        Load responses from jsonl file\n",
    "        Format: {id: idx, uuid: uuid, response: freetext response}, {...}\n",
    "        '''\n",
    "        filepath = os.path.join(RESPONSES_DIR, filename)\n",
    "        N = 5\n",
    "        with open(filepath, \"r\") as f:\n",
    "            responses = [json.loads(line) for line in f.readlines()]\n",
    "            if not full_run:\n",
    "                responses = responses[:N]\n",
    "        \n",
    "        return responses\n",
    "    \n",
    "def load_template(template_version:str):\n",
    "        template_filepath = os.path.join(\n",
    "              PROJECT_DIR,\n",
    "              \"prompt_templates/output-parsing_templates.json\"\n",
    "            )\n",
    "        with open(template_filepath, \"r\") as f:\n",
    "            templates = json.load(f)\n",
    "            template = templates[template_version]\n",
    "        \n",
    "        return template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import PromptTemplate\n",
    "import output\n",
    "\n",
    "responses = load_freetext_responses(\"freetext_turbo_700dev_14081857.jsonl\")\n",
    "\n",
    "template = PromptTemplate.from_template(load_template(\"v3\"))\n",
    "    \n",
    "output_parser, format_instructions = output.build_parser(\n",
    "    schemas_version=\"v3\",\n",
    "    parser_type=\"structured\",\n",
    "    only_json=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "24f04be45e9f43ed8d9d6b5a653b1a87",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model on device: cuda | cuda:0\n"
     ]
    }
   ],
   "source": [
    "model, tokenizer = codellama.load_codellama()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s>[INST] <<SYS>>\n",
      "Transform the free text into structured json, following the instructions and schema provided. Do not add information that is not from the original text. The text is the response to a multiple choice question, which includes the reasoning before choosing a final answer. Sometimes the deliberation does not lead to an answer being chosen as the final answer.\n",
      "\n",
      "The output should be a markdown code snippet formatted in the following schema, including the leading and trailing \"```json\" and \"```\":\n",
      "\n",
      "```json\n",
      "{\n",
      "\t\"answer_letter\": string  // The letter corresponding to the final answer. For example, if the text mentions 'Answer G) penguing is the most likely', then this field is 'G'. If multiple final answers are given, such as 'G' and 'U', then this field is 'G,U'. If a final answer is not chosen, then this field is an empty string.\n",
      "\t\"answer_text\": string  // The word or short phrase corresponding to the final answer. For example, if the text mentions 'Answer G) feeding penguings is the most likely', then this field is 'feeding penguins'. In case of multiple final answers, they are separate by a comma. If the answer_letter field is empty, then this field is an empty string.\n",
      "}\n",
      "```\n",
      "<</SYS>>\n",
      "\n",
      "A) Completing the job is one aim that people have at work. This means finishing the tasks assigned to them within the given time frame and meeting the expectations of the employer.\n",
      "\n",
      "B) Learning from each other is another aim that people have at work. This means interacting with colleagues and gaining new skills and knowledge that can help in personal and professional growth.\n",
      "\n",
      "C) Killing animals is not an aim that people have at work unless they work in specific industries like hunting or meat processing.\n",
      "\n",
      "D) Wearing hats is not an aim that people have at work unless it is a part of their uniform or dress code.\n",
      "\n",
      "E) Talking to each other is a common activity that happens at work, but it may not necessarily be an aim for everyone. Some people prefer to work independently and limit their interactions with others. \n",
      "\n",
      "Therefore, the most common aims that people have at work are completing the job and learning from each other. [/INST]\n"
     ]
    }
   ],
   "source": [
    "sys_prompt = template.template.split(\"\\n\\n\")[0] + \"\\n\\n\" + format_instructions\n",
    "# if examples, add to sys_prompt\n",
    "\n",
    "user_message = responses[1]['text']\n",
    "\n",
    "import prompting\n",
    "\n",
    "llama_prompt = prompting.get_llama_prompt(\n",
    "    sys_prompt=sys_prompt,\n",
    "    user_message=user_message\n",
    ")\n",
    "\n",
    "print(llama_prompt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mount/studenten-temp1/users/dpgo/xai-thesis/thesis-venv/lib64/python3.10/site-packages/transformers/generation/configuration_utils.py:362: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/mount/studenten-temp1/users/dpgo/xai-thesis/thesis-venv/lib64/python3.10/site-packages/transformers/generation/configuration_utils.py:367: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.5` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ```json\n",
      "{\n",
      "\t\"answer_letter\": \"A,B\",\n",
      "\t\"answer_text\": \"completing the job, learning from each other\"\n",
      "}\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "inputs = tokenizer(llama_prompt, return_tensors=\"pt\").to(device)\n",
    "\n",
    "output = model.generate(\n",
    "    input_ids=inputs['input_ids'],\n",
    "    attention_mask=inputs['attention_mask'],\n",
    "    temperature=0.1,\n",
    "    max_new_tokens=50,\n",
    "    do_sample=False,\n",
    "    top_p=0.5,\n",
    ")\n",
    "output = output[0].to('cpu')\n",
    "output_decoded_llama = tokenizer.decode(output[inputs['input_ids'].shape[1]:], skip_special_tokens=True)\n",
    "\n",
    "print(output_decoded_llama)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ```json\n",
      "{\n",
      "\t\"answer_letter\": \"A\",\n",
      "\t\"answer_text\": \"bank\"\n",
      "}\n",
      "```\n",
      "{'answer_letter': 'A', 'answer_text': 'bank'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ```json\n",
      "{\n",
      "\t\"answer_letter\": \"A,B\",\n",
      "\t\"answer_text\": \"completing the job, learning from each other\"\n",
      "}\n",
      "```\n",
      "{'answer_letter': 'A,B', 'answer_text': 'completing the job, learning from each other'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " {\n",
      "\t\"answer_letter\": \"B\",\n",
      "\t\"answer_text\": \"bookstore\"\n",
      "}\n",
      "{'answer_letter': 'B', 'answer_text': 'bookstore'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ```json\n",
      "{\n",
      "\t\"answer_letter\": \"A\",\n",
      "\t\"answer_text\": \"fast food restaurant\"\n",
      "}\n",
      "```\n",
      "{'answer_letter': 'A', 'answer_text': 'fast food restaurant'}\n",
      " ```json\n",
      "{\n",
      "\t\"answer_letter\": \"A,E\",\n",
      "\t\"answer_text\": \"midwest, Illinois\"\n",
      "}\n",
      "```\n",
      "{'answer_letter': 'A,E', 'answer_text': 'midwest, Illinois'}\n"
     ]
    }
   ],
   "source": [
    "for response in responses:\n",
    "    #sys prompt stays the same\n",
    "    user_message = response['text']\n",
    "    llama_prompt = prompting.get_llama_prompt(\n",
    "        sys_prompt=sys_prompt,\n",
    "        user_message=user_message\n",
    "    )\n",
    "    inputs = tokenizer(\n",
    "        llama_prompt,\n",
    "        return_tensors=\"pt\"\n",
    "    ).to(device)\n",
    "    output = model.generate(\n",
    "        input_ids=inputs['input_ids'],\n",
    "        attention_mask=inputs['attention_mask'],\n",
    "        temperature=0.1,\n",
    "        max_new_tokens=50,\n",
    "        do_sample=False,\n",
    "        top_p=0.5,\n",
    "    )\n",
    "    output = output[0].to('cpu')\n",
    "    output_decoded = tokenizer.decode(\n",
    "        output[inputs['input_ids'].shape[1]:],\n",
    "        skip_special_tokens=True\n",
    "    )\n",
    "    print(output_decoded)\n",
    "    \n",
    "    \n",
    "    output_parsed = output_parser.parse(output_decoded)\n",
    "    print(output_parsed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.add_special_tokens({'pad_token': '[PAD]'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !! batch generate() triggers CUDA error which kills the kernel\n",
    "batch_inputs = tokenizer(\n",
    "    [prompting.get_llama_prompt(\n",
    "        sys_prompt=sys_prompt,\n",
    "        user_message=response['text']\n",
    "    ) for response in responses[:2]],\n",
    "    return_tensors=\"pt\",\n",
    "    padding='longest',\n",
    "    truncation=True,\n",
    "    max_length=2048\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 504])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_inputs['input_ids'].shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis-venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
