{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import LlamaCpp\n",
    "from langchain import PromptTemplate, LLMChain\n",
    "from langchain.callbacks.manager import CallbackManager\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"Question: {question}\n",
    "\n",
    "Answer: Let's work this out in a step by step way to be sure we have the right answer.\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"question\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Callbacks support token-wise streaming\n",
    "callback_manager = CallbackManager([StreamingStdOutCallbackHandler()])\n",
    "# Verbose is required to pass to the callback manager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"/Users/q616967/Workspace/llama.cpp/llama-2-13b-chat.ggmlv3.q4_0.bin\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama.cpp: loading model from /Users/q616967/Workspace/llama.cpp/llama-2-13b-chat.ggmlv3.q4_0.bin\n",
      "llama_model_load_internal: format     = ggjt v3 (latest)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 512\n",
      "llama_model_load_internal: n_embd     = 5120\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 40\n",
      "llama_model_load_internal: n_head_kv  = 40\n",
      "llama_model_load_internal: n_layer    = 40\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: n_gqa      = 1\n",
      "llama_model_load_internal: rnorm_eps  = 1.0e-06\n",
      "llama_model_load_internal: n_ff       = 13824\n",
      "llama_model_load_internal: freq_base  = 10000.0\n",
      "llama_model_load_internal: freq_scale = 1\n",
      "llama_model_load_internal: ftype      = 2 (mostly Q4_0)\n",
      "llama_model_load_internal: model size = 13B\n",
      "llama_model_load_internal: ggml ctx size =    0.11 MB\n",
      "llama_model_load_internal: mem required  = 7349.72 MB (+  400.00 MB per state)\n",
      "llama_new_context_with_model: kv self size  =  400.00 MB\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: using MPS\n",
      "ggml_metal_init: loading '/Users/q616967/Workspace/bmw/xai_sofia_casadei_master_thesis/thesis-venv/lib/python3.10/site-packages/llama_cpp/ggml-metal.metal'\n",
      "ggml_metal_init: loaded kernel_add                            0x12f947b10\n",
      "ggml_metal_init: loaded kernel_add_row                        0x12f947d70\n",
      "ggml_metal_init: loaded kernel_mul                            0x12f947fd0\n",
      "ggml_metal_init: loaded kernel_mul_row                        0x12f948230\n",
      "ggml_metal_init: loaded kernel_scale                          0x12f948490\n",
      "ggml_metal_init: loaded kernel_silu                           0x12f9486f0\n",
      "ggml_metal_init: loaded kernel_relu                           0x12f948950\n",
      "ggml_metal_init: loaded kernel_gelu                           0x12f948bb0\n",
      "ggml_metal_init: loaded kernel_soft_max                       0x12f948e10\n",
      "ggml_metal_init: loaded kernel_diag_mask_inf                  0x12f949070\n",
      "ggml_metal_init: loaded kernel_get_rows_f16                   0x12f949770\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_0                  0x12f9499d0\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_1                  0x12f949c30\n",
      "ggml_metal_init: loaded kernel_get_rows_q2_K                  0x12f949e90\n",
      "ggml_metal_init: loaded kernel_get_rows_q3_K                  0x12f94a0f0\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_K                  0x12f94a350\n",
      "ggml_metal_init: loaded kernel_get_rows_q5_K                  0x12f94a5b0\n",
      "ggml_metal_init: loaded kernel_get_rows_q6_K                  0x12f94a810\n",
      "ggml_metal_init: loaded kernel_rms_norm                       0x12f94aa70\n",
      "ggml_metal_init: loaded kernel_norm                           0x12f94acd0\n",
      "ggml_metal_init: loaded kernel_mul_mat_f16_f32                0x12f94af30\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_0_f32               0x12f94b190\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_1_f32               0x12f94b3f0\n",
      "ggml_metal_init: loaded kernel_mul_mat_q2_K_f32               0x12f94b650\n",
      "ggml_metal_init: loaded kernel_mul_mat_q3_K_f32               0x12f94b8b0\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_K_f32               0x12f94bb10\n",
      "ggml_metal_init: loaded kernel_mul_mat_q5_K_f32               0x12f94bd70\n",
      "ggml_metal_init: loaded kernel_mul_mat_q6_K_f32               0x12f94bfd0\n",
      "ggml_metal_init: loaded kernel_rope                           0x12f94c230\n",
      "ggml_metal_init: loaded kernel_alibi_f32                      0x12f94c490\n",
      "ggml_metal_init: loaded kernel_cpy_f32_f16                    0x12f94c6f0\n",
      "ggml_metal_init: loaded kernel_cpy_f32_f32                    0x12f94c950\n",
      "ggml_metal_init: loaded kernel_cpy_f16_f16                    0x12f94cbb0\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize = 10922.67 MB\n",
      "ggml_metal_init: hasUnifiedMemory             = true\n",
      "ggml_metal_init: maxTransferRate              = built-in GPU\n",
      "llama_new_context_with_model: max tensor size =    87.89 MB\n",
      "ggml_metal_add_buffer: allocated 'data            ' buffer, size =  6984.06 MB, (14737.81 / 10922.67), warning: current allocated size is greater than the recommended max working set size\n",
      "ggml_metal_add_buffer: allocated 'eval            ' buffer, size =    12.00 MB, (14749.81 / 10922.67), warning: current allocated size is greater than the recommended max working set size\n",
      "ggml_metal_add_buffer: allocated 'kv              ' buffer, size =   402.00 MB, (15151.81 / 10922.67), warning: current allocated size is greater than the recommended max working set size\n",
      "ggml_metal_add_buffer: allocated 'scr0            ' buffer, size =   162.00 MB, (15313.81 / 10922.67), warning: current allocated size is greater than the recommended max working set size\n",
      "ggml_metal_add_buffer: allocated 'scr1            ' buffer, size =   192.00 MBAVX = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | VSX = 0 | \n",
      ", (15505.81 / 10922.67), warning: current allocated size is greater than the recommended max working set size\n"
     ]
    }
   ],
   "source": [
    "n_gpu_layers = 1  # Change this value based on your model and your GPU VRAM pool.\n",
    "    # 1 is enough for Metal installation\n",
    "n_batch = 512  # Should be between 1 and n_ctx, consider the amount of VRAM in your GPU.\n",
    "    # defaults to 8\n",
    "\n",
    "# Make sure the model path is correct for your system!\n",
    "llm = LlamaCpp(\n",
    "    model_path=model_path,\n",
    "    n_gpu_layers=n_gpu_layers,\n",
    "    n_batch=n_batch,\n",
    "    n_threads=8,\n",
    "    f16_kv=True,  # MUST set to True, otherwise you will run into problem after a couple of calls\n",
    "    temperature=0.0,\n",
    "    max_tokens=-1,\n",
    "    repeat_penalty=1,\n",
    "    streaming=True,\n",
    "    callback_manager=callback_manager,\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ggml_metal_free: deallocating\n"
     ]
    }
   ],
   "source": [
    "llm_chain = LLMChain(prompt=prompt, llm=llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "1. The question is asking about one of many ways to practice your...\n",
      "\n",
      "2. The answer choices are:\n",
      "\n",
      "A: literacy\n",
      "\n",
      "B: knowing how to read\n",
      "\n",
      "C: money\n",
      "\n",
      "D: buying\n",
      "\n",
      "E: money bank\n",
      "\n",
      "3. Based on the context of the question, which of these options makes the most sense?\n",
      "\n",
      "Given that the question is about reading a newspaper, the most logical answer choice is...\n",
      "\n",
      "Answer: A: literacy.\n",
      "\n",
      "Explanation: Reading a newspaper is an activity that involves literacy, which is the ability to read and write. Therefore, reading a newspaper is one of many ways to practice your literacy skills."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time = 14118.58 ms\n",
      "llama_print_timings:      sample time =    32.87 ms /   151 runs   (    0.22 ms per token,  4593.30 tokens per second)\n",
      "llama_print_timings: prompt eval time = 14118.50 ms /    78 tokens (  181.01 ms per token,     5.52 tokens per second)\n",
      "llama_print_timings:        eval time = 22526.54 ms /   150 runs   (  150.18 ms per token,     6.66 tokens per second)\n",
      "llama_print_timings:       total time = 37216.28 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n\\n1. The question is asking about one of many ways to practice your...\\n\\n2. The answer choices are:\\n\\nA: literacy\\n\\nB: knowing how to read\\n\\nC: money\\n\\nD: buying\\n\\nE: money bank\\n\\n3. Based on the context of the question, which of these options makes the most sense?\\n\\nGiven that the question is about reading a newspaper, the most logical answer choice is...\\n\\nAnswer: A: literacy.\\n\\nExplanation: Reading a newspaper is an activity that involves literacy, which is the ability to read and write. Therefore, reading a newspaper is one of many ways to practice your literacy skills.'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question = \"\"\"\n",
    "Reading newspaper one of many ways to practice your what? \n",
    "Choices: -A: literacy, -B: knowing how to read, -C: money, -D: buying, -E: money bank\n",
    "\"\"\"\n",
    "\n",
    "llm_chain.run(question) # 1 input variable call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis-venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
